{
  "title": "Preserving privacy in medical images while still enabling AI-driven research: A comprehensive review",
  "sections": [
    {
      "title": "Abstract",
      "content": "The application of AI algorithms in medical image processing requires access to a large amount of data containing protected health information (PHI). Preserving individuals' privacy is not only an ethical issue, but it is also dictated by personal privacy laws such as HIPAA or GDPR. Health authorities and hospitals should be aware that is not possible to fully anonymize medical images without losing their research utility, meaning that some level of risk for potentially reidentifying patient information will be present in any case. On the other hand, researchers and software developers should be informed about de-identification or anonymization approaches and should consider them as part of any solution. Review papers published in this area are mainly focused on preserving privacy in structured medical data or exploring defense mechanisms and approaches against adversarial attacks. This paper takes into consideration unstructured medical data and provides an overview of the: techniques and tools adopted for medical image de-identification or anonymization; faced limitations in ensuring patient privacy; and researchers' future directions."
    },
    {
      "title": "NTRODUCTION",
      "content": "The use of AI has an increasing impact in the field of medicine and in terms of scientific research, medical images play an important role . Such data is collected by health institutions on a daily basis and stored into patient's health records. The latter, are not exposed for public use due to privacy issues related to personal health information disclosure. On the other hand, a vast amount of image processing AI models can be built, trained and tested only with the help of large medical image datasets .\nThe large amount of data needed, the complex nature of AI models, and private health information preservation are some of the main factors that complicate the process of medical image data sharing . Medical AI-driven research is striving for better solutions while keeping patients' privacy intact . To ease things up, negotiation with respective data owners is required, but this is not an easy task, especially when performed under privacy regulation laws .\nHowever, seen from another point of view, through the presence of data regulation laws (HIPAA, GDPR, CCPA, etc.) the process of accessing data for research purposes, is facilitated. These laws suggest the use of technical solutions to de-identify or anonymize PHI before using the data that contains it and incentivize the development of new techniques to protect privacy .\nFor a better understanding of the de-identification or anonymization techniques applied by researchers to unstructured medical data, particularly to medical images, a review is conducted. Our focus will be only in the two forementioned tasks employed to preserve privacy in this type of data. Additionally, due to the complexity of these operations, authors have to deal with numerous challenges and limitations.\nOur review aims to answer the following questions:\n RQ1: Does the state-of-art suggest any standard deidentification/anonymization approach for preserving privacy in medical images?  RQ2: What are the limitations and challenges faced by the authors during medical image deidentification/anonymization?\nThe subsequent sections of this study are organized as follows: Section 2 presents a brief state-of-art summary. Section 3 describes the methodology used to conduct the research. Section 4 interprets the results obtained from the selected relevant papers. Section 5 draws the conclusions obtained and gives suggestions about steps that can be considered in the future.\nII. LITERATURE REVIEW Medical images are an important tool for medical diagnosis and are vital to increase the performance accuracy of AI tasks, such as image classification, image segmentation, transfer learning, etc. To be able to use these images as an input to AI algorithms, the personal data that they contain must be protected. However, before trying to prevent personal health information disclosure, one should first understand what type of data is considered to be personal. A set of direct identifiers that can be found in images and should be de-identified are specified by HIPAA privacy rules .\nBesides de-identification and anonymization, other privacy protection approaches can be undertaken. Examples of such methods are federated learning, differential privacy and homomorphic encryption . In the literature, these tasks are performed in cases where privacy breaches can occur and there is a need to protect diagnostic models published to third parties from adversarial attacks .\nAccording to Feng et al. the current defense approaches have a bad impact to the functionality of AI models. Because of that they decided to create a Fake Gradient defense framework, which encrypts the model's output to protect it from attackers. It is focused on image classification models. Jarin & Eshete introduced PRICURE system to prevent membership inference attacks by maintaining the accuracy of the models intact. It combines secure multi-party computation and differential privacy to enable private multiple model prediction. Additionally, Nikolaidis et al. have studied the ability of implicit learning from visually unrealistic stimuli to guarantee resilience against membership inference attacks .\nAbuadbba et al. have studied whether 1D CNN models can be trained while preserving privacy through split learning procedure. This is performed by splitting the model into client side and server side respectively. While the client processes the raw data, the server is not able to directly access it. Meanwhile, Santos & Rocha mentioned the problems of content-based, network and DOS or DDOS attacks during data preparation stage.\nProtecting demographics and diagnosis codes is important considering the possibility that they give to researchers to conduct medical studies. Existing studies are not able to guard identity disclosure, ensure useful anonymization and minimize the loss of information at the same time. For this reason, Poulis et al. proposed a new approach by applying (k, k m )anonymity in datasets containing demographic and diagnoses codes.\nTo improve patient care, sharing health information data is crucial and challenging at the same time. De Kok et al. explored accessible healthcare databases to dictate appropriate sharing. A seven-recommendation approach was formulated by the authors to guide future open healthcare data sharing initiatives.\nAbouelmehdi et al. have discussed about privacy and security concerns present in big health data. Despite the fact that they have reviewed encryption and anonymization methods applied to general healthcare data, their study states that techniques like hiding a needle in a haystack, attribute-based encryption access control, homomorphic encryption and storage path encryption are also present.\nMajeed proposed an anonymization scheme that aims to guarantee data privacy, but mainly to ensure protection of identity from adversarial attacks. His proposed solution was focused on general electronic health records in the clouds."
    },
    {
      "title": "METHODOLOGY",
      "content": "A comprehensive protocol-driven review was conducted in three large databases (ScienceDirect, ACM, PubMed). Fig. 1 shows in details the reproducible systematic procedure followed by the authors."
    },
    {
      "title": "Inclusion Criteria",
      "content": "The search query performed in the selected databases was: medical image AND (anonymization OR anonymisation OR deidentification OR de-identification) AND (privacy OR confidentiality OR data protection). We considered only papers fulfilling the following inclusion criteria:  Articles following a specific de-identification or anonymization approach;"
    },
    {
      "title": "Exclusion Criteria",
      "content": "From the final analysis were excluded duplicates and:\n Articles performing de-identification/anonymization process in structured medical data instead of medical images (unstructured medical data);\n Non-experimental articles that do not give details about the de-identification/anonymization process;\n Articles focusing on security attacks and defense mechanisms instead of patient's privacy preservation based on what data privacy regulations (HIPAA, GDPR, etc.) require;\nIV. DISCUSSIONS From the screening procedure, 41 papers were assessed for full-text relevance and only 8 papers were found eligible for our review. With the aim of answering the research questions forementioned in the introduction section, we have analyzed the relevant studies based on: the de-identification or anonymization process performed; the specific data modality used; the challenges they underwent and the suggested future directions."
    },
    {
      "title": "De-identification or Anonymization Approach",
      "content": "Results shown in Table 1 give an outline of the approaches adopted by the relevant papers to preserve patient's privacy in medical image data, while still enabling research. The first thing to note is that all the studies have performed de-identification instead of anonymization. Besides that, they have all adopted an open-source solution and the URLs provided under the \"Solution URL\" column indicate the available source code.\nIt is important to highlight the fact that these two terms were often used interchangeably in the selected papers. Through anonymization we make sure that personal information cannot be used to re-identify a patient, meaning that this process once performed, is irreversible. By contrast, de-identification makes re-identification impossible unless individuals are authorized to do so .\nRegarding the de-identification solution, half of the relevant studies have employed a RSNA CTP pipeline , which is a solution that can be personalized according to particular requirements. Moreover, it is utilized across diverse image modalities, but mostly in DICOM format data. Another preferred solution is distorting all or partial facial features using an image de-facer program. The latter is applied on head or brain MRI image modalities only. Moreover, generating synthetic MRIs using an auxiliary classifier Generative Adversarial Network instead of using real data or simply de-identifying the DICOM header and checking further for pixel burned-in PHI are considered as possible solutions as well."
    },
    {
      "title": "Limitations and Challenges",
      "content": "Clinical picture archives and communication systems (PACS), use DICOM as a standard image format for different types of medical images. It is common for manufacturers or imaging system vendors, who are responsible for the documentation of PHI, to exclude some private attributes from the DICOM conformance statements. Sometimes, they encode acquisition parameters into private attributes or they do not use attributes fields for their intended purposes. PHI might be nested in different DICOM levels and as a consequence, it can be ignored if only the first DICOM level gets scanned. In addition, it is often difficult to find the conformance statement itself because vendor's model and SW version information are removed by mistake during image submission.\nFurthermore, the de-identification system works separately from the original image center, image pixels might contain private data and DICOM attributes can vary based on image modalities . More limitations come with the fact that different authorities (GDPR, HIPAA, etc.) specify different regulations, which complicates the process standardization. DICOM de-identification solutions need to be evaluated and checked for quality before being deployed, as well . When it comes to using a de-facer program for de-identification, the scientific utility of the images can be compromised if all facial features get removed, but on the other hand, re-identification can be easily achieved from disclosed features . Additionally, when generating a synthetic dataset is adopted as an alternative to de-identification, limited sample size and ad hoc computed privacy are challenges that need to be considered ."
    },
    {
      "title": "Future Directions",
      "content": "When dealing with the process of removing PHI from medical image data, we should be aware of the fact that various image modalities exist and different attributes might need different de-identification or anonymization strategies, such as randomizing, replacing, clearing or removing the attribute . Also, real datasets used to train privacy preservation systems are limited in terms of the features that they consider , while synthetic generated datasets do not always greatly resemble the real ones . These issues are part of the author's future plans.\nMeanwhile, for the available de-identified datasets, authors plan to: improve their management; optimize their deidentification by adding machine learning OCR method to remove PHI from pixel data; provide researchers more metadata and increase the dataset utility. Authors state that in the future they will further improve the secure data storage and increase the variety of medical image data of these datasets .\nHowever, there are plenty of limitations regarding image deidentification that arise because of the image nature itself, but the most important is input data quality. Since the DICOM format is a standard way to store and share medical images, a common process beginning from the first step of image submission, should be followed by different vendors or data owners. PHI must be correctly documented and conformance statements must be clearly defined."
    },
    {
      "title": "ABLE I. MEDICAL IMAGE DE-IDENTIFICATION/ANONYMIZATION",
      "content": "Ref."
    },
    {
      "title": "olution/Approach Privacy Process Opensource",
      "content": "Solution URL Data/Dataset ac-GAN model to generate synthetic data De-identification ✓ https://github.com/tcoroller/pGAN MRI dataset A two steps procedure:\n-De-identification at the image data source; -Pixel data de-identification;\nDe-identification ✓ https://csgitlab.ucd.ie/mldawn/dicom _de_identifier_public DICOM CT-scan images (NIMIS) Eyes, noses and ears were distorted from the images by using a Defacer SW; Header PHI removal through a function;\nDe-identification V. CONCLUSIONS Large image datasets are essential for training AI models, performing complex machine learning tasks, or simply conducting AI-driven research. This type of data contains sensitive information belonging to patients that need to be protected at all costs. Different techniques and approaches to preserve the privacy of individuals are currently available, but the majority of authors in the state-of-art, have adopted them for two main reasons: to preserve privacy in structured medical data; and to protect against adversarial attacks.\nIn this review paper authors have taken into consideration unstructured medical data, to be more precise, medical images. Because of their format, private information can be present not only in the visible parts but also hidden in the image pixel data. Given that, privacy protection, in this case, becomes an arduous task.\nThis study explores de-identification and anonymization techniques applied to medical images, as a means to preserve privacy while still being able to conduct accurate AI-driven research. From the results of our screening process, we wanted to understand if a standard de-identification or anonymization procedure is available in the existing literature. The relevant selected studies suggested different solutions, but the most adopted was the RSNA CTP pipeline. Two main reasons made the latter more preferred: the flexibility, since it could be easily customized; and the variety of imaging modalities that it could consider for de-identification."
    }
  ]
}