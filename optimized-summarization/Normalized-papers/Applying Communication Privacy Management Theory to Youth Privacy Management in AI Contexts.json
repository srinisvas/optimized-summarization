{
  "title": "Applying Communication Privacy Management Theory to Youth Privacy Management in AI Contexts",
  "sections": [
    {
      "title": "Abstract",
      "content": "The rapid integration of Artificial Intelligence (AI) technologies into the lives of young digital citizens has escalated privacy concerns and the need for critical examination. This study uses Communication Privacy Management (CPM) Theory to understand how youth and critical stakeholders navigate these concerns. A total of 306 participants were surveyed, comprising 146 AI professionals, 127 parents and educators and 33 youths (aged 16-19). Employing a mixed-methods approach, the research combined quantitative data from structured questionnaires with qualitative insights from open-ended responses. Descriptive statistics reveal distinct perspectives among different demographics regarding data ownership, education, transparency and trust, parental role and perceived risks and benefits associated with AI systems. Structural equation modelling identified key influences on youth privacy management, highlighting the significance of transparency and trust, education and awareness, and parental data sharing among AI professionals, parents, educators, and young digital citizens. The qualitative analysis further underscored unique concerns, emphasizing a lack of understanding and data misuse contributed to the feeling of helplessness shared by all stakeholders. This study underscores the importance of integrating diverse stakeholders' perspectives in the development of AI systems to address the complex challenges faced by youth. Recommendations include collaborative policymaking, implementing user-centric design practices, and enhancing privacy education to empower young digital citizens."
    },
    {
      "title": "INTRODUCTION",
      "content": "The evolution of Artificial Intelligence (AI) technologies has profoundly reshaped daily life, particularly for young digital citizens who are often at the forefront of adopting innovations , . AI-driven systems ranging from virtual assistants to social media algorithms and personalized learning platforms rely heavily on extensive data collection to provide personalized experiences and improved services. These technologies undoubtedly have benefits, but they also pose serious problems concerning data ownership, privacy, and ethical use of personal data .\nYoung digital citizens are commonly defined as digital natives growing up in a digital era where sharing personal information online over social media has become normalized. Many in this demographic have limited awareness of the complexities of data privacy and the potential long-term risks of their information disclosures. This lack of understanding leaves them particularly vulnerable to privacy breaches, identity theft and the misuse of their data. Compounding this issue, the lack of transparency and technical complexity of AI systems often hinder young users from making informed choices regarding their privacy. Even though AI is omnipresent in young digital citizens' lives, research exploring how they handle privacy issues within AI-driven environments is limited . The current state of the art tends to concentrate mainly on adult populations or broad privacy issues, ignoring the difficulties that young digital citizens encounter when engaging with AI-driven platforms. This emphasizes the necessity of looking into the variables affecting how youth handle their privacy to improve policy frameworks, educational initiatives, and AI design methodologies.\nTo address this knowledge gap, this study uses the Communication Privacy Management (CPM) Theory as a framework for understanding youth privacy management behaviors in AI contexts. CPM Theory emphasizes how individuals regulate the disclosure of personal information through key processes, including privacy ownership, control, rulemaking, and the management of boundary turbulence that occurs when privacy expectations are violated. This study specifically examines five variables: Data Ownership and Control (DOC), Parental Data Sharing (PDS), Perceived Risks and Benefits (PRB), Transparency and Trust (TT), and Education and Awareness (EA) . Together, these constructs provide a comprehensive view of the interactions between individual perceptions, parental roles, system transparency, and educational factors in shaping privacy management behaviors. While CPM Theory has been applied in other contexts, its relevance to youth privacy management within AI environments has not yet been fully explored.\nIn this study, we gathered survey data from 306 participants, including 146 AI developers and researchers, 127 parents and educators, and 33 young digital citizens between the ages of 16 and 19. The hypotheses produced from CPM Theory were tested and the interactions between the constructs were analyzed using Partial Least Squares Structural Equation Modeling (PLS-SEM) technique. This work intends to close a significant gap in the literature and offer practical suggestions for improving youth privacy management in AI contexts by fusing CPM Theory with empirical data. In addition to broadening the theoretical applicability of CPM Theory to contemporary technological contexts, this research provides insightful information for AI developers, educators, and policymakers.\nThe rest of the paper is organized as follows: the next section provides an overview of related works and background literature. The methodologies are outlined in section III. Section IV presents the results of our study. Section V provides the discussion with recommendations for ethical AI practices, limitations, and future research direction. Finally, Section VI concludes the paper."
    },
    {
      "title": "BACKGROUND AND RELATED WORKS",
      "content": ""
    },
    {
      "title": "Communication Privacy Management Theory",
      "content": "Developed by Sandra Petronio , Communication Privacy Management (CPM) Theory serves as a foundational framework for understanding how individuals make decisions about sharing or withholding private information. CPM emphasizes that people view private information as their possession, granting them the autonomy to control and regulate its disclosure based on personal privacy rules. These privacy rules are shaped by various factors, including cultural norms, motivations, assessments of risks and benefits, and contextual circumstances .\nThe theory outlines some fundamental key principles that are particularly relevant to comprehending how to maintain young digital citizens' privacy in AI-driven environments:\nPrivacy Ownership: People think they own their private information and are entitled to control how it is disclosed.\nPrivacy Control: Techniques are used to manage the accessibility and flow of personal data.\nPrivacy Rules: Established guidelines that take into account a variety of influencing elements serve as a reference for personal privacy decisions.\nPrivacy Boundaries: Symbolic boundaries that assist people regulate what they share by separating private information from public information.\nBoundary Coordination: When private information is shared with others, there is a process of negotiation to manage shared privacy boundaries.\nBoundary Turbulence: When privacy norms are broken, there are disturbances that result in disagreements or the need to renegotiate privacy boundaries."
    },
    {
      "title": "Constructs Being Examined",
      "content": "Table I provides the definitions of the five key constructs examined in our study: Data Ownership and Control (DOC), Parental Data Sharing (PDS), Perceived Risks and Benefits (PRB), Transparency and Trust (TT), and Education and Awareness (EA)."
    },
    {
      "title": "ONSTRUCTS AND DEFINITION",
      "content": ""
    },
    {
      "title": "onstruct Definition",
      "content": "Data Ownership and Control (DOC) , , , It is the degree to which young people have control over their personal data and engage in discussions about privacy.\nParental Data Sharing (PDS) , , It is the degree to which parents exercise their rights to share children's data and consider the implications of doing so.\nPerceived Risks and Benefits (PRB) , , It is the degree to which individuals perceive risks, ethical concerns, and benefits related to the use of personal data by AI systems. Transparency and Trust (TT) , , It is the degree to which transparency in data usage influences trust in AI systems.\nEducation and Awareness (EA) , , , It is the degree to which stakeholders are informed about privacy and ethical issues associated with AI.\nIn an era where AI systems increasingly collect and analyze personal information, youths' sense of ownership is often challenged by the opaque and complex nature of data practices , . A strong sense of ownership is linked to privacyprotective behaviors, empowering young individuals to establish boundaries and exercise autonomy over their data . However, the intricacy of AI systems and the lack of userfriendly privacy controls frequently undermine their ability to maintain such control . Studies reveal that youths often have a limited understanding of data ownership, which further hampers their ability to effectively manage their privacy in AIdriven environments .\nAs parental involvement can be instrumental in guiding and educating children about privacy risks, it also has the potential to inadvertently undermine youths' privacy management through the over-disclosure of personal information . Striking a balance between parental authority and youths' autonomy requires careful boundary coordination to ensure that privacy rights are respected. Open and effective communication between parents and their children fosters the negotiation of privacy boundaries, thereby minimizing the likelihood of boundary turbulence and associated conflicts .\nFurthermore, PRB plays a critical role in shaping how youth decide whether to disclose personal information to AI systems. According to CPM Theory, individuals weigh the potential benefits against the associated risks before making decisions about sharing private information . For youth, the perceived advantages, such as personalized content and improved user experiences, often outweigh the concerns of risks like data breaches or misuse, prompting them to share their data more readily . However, understanding how young digital citizens assess these risks and benefits is vital to developing effective strategies that support their privacy management. Studies reveal that young individuals tend to perceive risks at a lower level than adults, which can result in more permissive data-sharing behaviors .\nAdditionally, TT is pivotal in influencing how youth approach privacy management in AI systems. Transparent AI technologies that clearly communicate data collection and usage practices foster greater trust, encouraging users to share personal information with confidence . Trust acts as a critical factor in shaping how individuals set and negotiate privacy boundaries, determining their willingness to disclose information. Conversely, a lack of transparency can erode trust, making users hesitant to engage or share data. Enhancing transparency in AI systems not only mitigates perceived risks but also establishes a foundation for stronger, more trusting relationships between youth and technology .\nEA also plays a critical role in equipping youth with the tools needed for effective privacy management. Within the framework of CPM, knowledge and understanding of privacy issues are essential for developing informed privacy rules and making deliberate decisions about sharing personal information . Educational initiatives that focus on increasing awareness about AI technologies, data handling practices, and privacy rights empower youth to assert greater control over their personal information, reducing their susceptibility to privacy violations . Research highlights that higher levels of digital literacy are associated with more cautious and thoughtful online behavior among youth, reinforcing the importance of education in promoting responsible digital engagement ."
    },
    {
      "title": "Application of CPM to Youth Privacy in AI Contexts",
      "content": "Applying CPM Theory to youth privacy management in AI environments involves a comprehensive integration of personal, social, and technological dimensions. This theoretical framework offers insights into how young individuals navigate privacy by examining key factors such as control over their data, parental control, risk and benefit assessments, transparency in AI systems, and awareness. Despite its significant applicability, prior research has predominantly addressed privacy concerns among adult populations within general contexts, leaving a gap in understanding the specific experiences and challenges faced by youth in AI-driven interactions . This study bridges that gap by employing CPM to analyze empirical data, shedding light on the unique privacy behaviors of young digital citizens and providing valuable contributions to both theoretical discourse and actionable policy design."
    },
    {
      "title": "METHODOLOGY",
      "content": ""
    },
    {
      "title": "Research Goal and Questions",
      "content": "Our research's main goal was to apply CPM Theory to comprehend how young digital citizens maintain their privacy with regard to AI technologies by examining the relationships among five validated constructs: DOC, PDS, PRB, TT, and EA. We aimed to address the following research questions, which serve as the basis for the conceptual framework illustrated in Fig. 1: • RQ1: How do youth develop privacy rules regarding their personal data when interacting with AI technologies?\n• RQ2: What role do parents play in shaping youths' privacy boundaries and data-sharing behaviors?\n• RQ3: How do perceived risks and benefits influence youths' decisions to disclose personal information to AI systems?\n• RQ4: In what ways do transparency and trust in AI systems affect youths' privacy management?\n• RQ5: How do education and awareness about AI and data privacy impact youths' control over their personal information?\nFig. 1 provides a visual representation of how these research questions are mapped to the five constructs to collect insights from stakeholders. These variables, informed by the research questions, converge to inform actionable insights for Ethical AI Development. The definitions of these constructs are provided in Table I, which details their scope and focus within the study."
    },
    {
      "title": "Research Model and Hypotheses",
      "content": "Based on the findings from the literature review, we have developed thirteen research hypotheses aligned with different research questions to examine the constructs detailed in Table I.\nThe hypotheses associated with RQ1 are outlined below:\n• H1: Education and Awareness (EA) positively influences Data Ownership and Control (DOC).\n• H2: Perceived Risks and Benefits (PRB) positively influences Data Ownership and Control (DOC).\n• H3: Transparency and Trust (TT) positively influences Data Ownership and Control (DOC).\n• H4: Perceived Risks and Benefits (PRB) mediates the relationship between Education and Awareness (EA) and Data Ownership and Control (DOC).\n• H5: Perceived Risks and Benefits (PRB) mediates the relationship between Transparency and Trust (TT) and Data Ownership and Control (DOC).\nThe hypotheses associated with RQ2 are outlined below:\n• H6: Parental Data Sharing (PDS) positively influences Data Ownership and Control (DOC).\n• H7: Parental Data Sharing (PDS) positively influences Education and Awareness (EA).\n• H8: Education and Awareness (EA) mediates the relationship between Parental Data Sharing (PDS) and Data Ownership and Control (DOC).\nThe hypotheses associated with RQ3 are outlined below: Note: While PDD is not explicitly listed as a construct, it can be considered an outcome variable related to DOC.\nThe hypotheses associated with RQ4 are outlined below:\n• H10: Transparency and Trust (TT) positively influences Perceived Risks and Benefits (PRB).\n• H11: Transparency and Trust (TT) has an indirect effect on Data Ownership and Control (DOC) through Perceived Risks and Benefits (PRB).\nThe hypotheses associated with RQ5 are outlined below:\n• H12: Education and Awareness (EA) positively influences Perceived Risks and Benefits (PRB).\n• H13: Education and Awareness (EA) has both a direct and indirect effect on Data Ownership and Control (DOC) through Perceived Risks and Benefits (PRB)."
    },
    {
      "title": "Research Design",
      "content": "The present study received ethics approval from the Vancouver Island University Research Ethics Board (VIU-REB). The approval with reference number #103116 was given for behavioral application/amendment forms, consent forms and questionnaires. We conducted a pilot study with six participants, including members of empirical research specialists from the University of Saskatchewan and Vancouver Island University. The pilot study aimed to assess the feasibility and duration of the research approach and refine the study design. Participants provided general feedback on the questionnaire which informed modifications and restructuring of the final survey questionnaires. The revised research model was then assessed by gathering survey data. We recruited participants through flyers, emails, personal networks, and on social networking sites, LinkedIn, and Reddit. To reach young digital citizens, we spoke with several school districts for their assistance in distributing our survey to their high-school students. Participation was entirely voluntary and did not receive any form of compensation. The participants had to read and accept a consent form to participate in the study, by submitting the consent form before starting the questionnaire participants were indicating they understood the conditions of participation in the study outlined in the consent form. We conducted online surveys through Microsoft Forms by requesting each participant to respond to the questionnaire based on our three designated demographics: AI Researchers and Developers, Teachers and Parents, and Youth aged 16-19.\nThe survey instruments are adapted from constructs validated in prior studies , , , , , , , , , , , , , , , , We measured responses to the items, excluding qualitative items, on a 5-scale Likert scale. Notably, to ensure consistency in outcomes, we reversed the scale for items in PRB for AI Researchers and Developers and swapped items 1 and 2 in PDS for Young Digital Citizens to align contextually with the items in PRB and PDS for the other demographics. The open-ended questions and 2 indicators from PRB were used for qualitative analysis, while the remaining items were used for quantitative analysis."
    },
    {
      "title": "Participants demographics",
      "content": "A total of 326 participants took part in the study: 132 were parents and/or educators, 153 were AI professionals, and 41 were young digital citizens (aged IV. RESULTS\nThis study expands our previous research by focusing specifically on young digital citizens, exploring their unique perspectives on privacy in AI systems alongside the insights gathered from parents/educators and AI professionals. This approach allows us to delve deeper into the privacy concerns and awareness levels of the younger demographic, highlighting their specific challenges and needs in the context of AI.\nFor data processing, Microsoft Excel was employed to manage the collected data through descriptive statistics. We consolidated data from various demographic groupsincluding educators, parents, AI professionals, and young digital citizens-into a single dataset for streamlined analysis. The analysis was conducted using a partial least squares structural equation modeling (PLS-SEM) approach via smartPLS software . PLS-SEM is a robust method commonly used to estimate path coefficients in structural models, widely recognized in numerous studies , . The SEM as suggested by includes the testing of measurement models (exploratory factor analysis, internal consistency, convergent validity, Dillon-Goldstein's rho) and the structural model (regression analysis). We employed the path-weighing structural model scheme in smartPLS which provides the highest R 2 values for dependent latent variables.\nAdditionally, a nonparametric bootstrapping procedure was utilized to assess the statistical significance of the PLS-SEM results. Bootstrapping is a resampling technique that creates an empirical sampling distribution by drawing repeated samples with replacements from the original data set. For our analysis, 5,000 subsamples were generated, and a two-tailed test was conducted at a significance level of 0.1.\nWe also conducted a thematic analysis of open-ended questions to identify common themes expressed by participants."
    },
    {
      "title": "Descriptive Statistics",
      "content": "Our quantitative survey used a 5-point Likert scale to compare mean responses across five key constructs, as shown in Fig. 2. Parents/educators and AI developers/researchers had similar mean scores, while young digital citizens reported lower means, indicating a gap in perceptions. Data Ownership and Control (DOC) was prioritized by all groups: parents/educators scored 3.75, researchers/developers 3.95, and youth 3.32, highlighting the importance of user autonomy over personal data.\nFor Transparency and Trust (TT), parents/educators averaged 3.46 and researchers/developers 3.49, while youth scored lower at 2.95, suggesting less trust in AI systems. Parental Data Sharing (PDS) received low scores across groups: 2.94 for parents/educators, 2.36 for researchers/developers, and 1.91 for youth indicating reluctance towards data-sharing behaviors, especially among AI professionals who rated it lower than parents/educators. Perceived Risks and Benefits (PRB) had the highest ratings: parents/educators 3.88, researchers/developers 4.43, and youth 3.61, showing recognition of the ethical implications of AI data practices. Researchers/developers emphasized this aspect more, reflecting their awareness of the broader impacts. Education and Awareness (EA) revealed the largest gap: parents/educators rated it at 3.43, researchers/developers at 4.16, and youth at 2.94. This suggests adults value knowledge of AI and privacy, while youth may lack awareness of its importance.\nOverall, while adults agree on user control, transparency, and engagement with AI, youth show lower trust and awareness, highlighting the need for targeted interventions to bridge this gap."
    },
    {
      "title": "Measurement Models",
      "content": "We evaluated the measurement model using exploratory factor analysis to assess the internal consistency, reliability, and validity of the constructs.\nExploratory Factor Analysis: For exploratory factor analysis, we first checked the factor loadings of individual items shown in Table IV, to see how each variable loaded on its own construct over the other respective constructs. Factor loadings greater than 0.60 can be considered as significant according to . In our study, all the indicators in the measurement model had a factor loading of value greater than 0.60 except for item 2 in the contrust Trust and Transparency (TT), and item 1 in the Data Ownership and Control (DOC) construct. Item tt2 had a low loading value of 0.344 which would suggest that it be avoided in the model. Although we did use the validated constructs, our exploratory analysis showed that tt2 had a weak influence on Trust and Transparency. Item doc1 had a factor loading value of 0.536, which is just under the significant level of 0.60, which is still deemed moderately acceptable ."
    },
    {
      "title": "Construct reliability and validity",
      "content": "We assessed convergent validity for each construct by calculating Average Variance Extracted (AVE) and Composite Reliability (CR) from factor loadings (see Table V). AVE should exceed 0.50, indicating that 50% of the variance in the items is captured by the hypothesized constructs, and CR should be above 0.75 . In our study, AVE exceeded 0.50 for all constructs except for Trust and Transparency (TT) and Data Ownership and Control (DOC), which also had CR values slightly below 0.75. TT, with an AVE of 0.449 and CR of 0.686, suggests that it did not capture significant variance to converge into a single construct. . DOC achieved a rho_A of 0.510, suggesting moderate reliability. EA scored 0.622, also reflecting moderate reliability. PDS scored 2.062, which is above the acceptable range. This indicates significant variation within the response items, most likely due to the combination of all three demographics, students, parents/educators, and AI developers/researchers, who may have differing opinions. In contrast, PRB exhibited a low score of 0.197, suggesting poor reliability and questioning its appropriateness for inclusion in further analysis. Finally, TT achieved a rho_A of 0.529, indicating moderate reliability.\nOverall, while most constructs exhibited acceptable levels of reliability, the PRB and PDS constructs may require reconsideration in future analyses due to their scores."
    },
    {
      "title": "Structural Models",
      "content": "The results of our PLS-SEM analysis are depicted in Fig. 3, featuring coefficients of determination (R 2 's), path coefficients (β), and p-values. The R 2 values indicate the variance explained in each construct by its antecedents, while the β values measure the strength of relationships between constructs. P-values assess the statistical significance of these relationships. According to Chin's guideline , , a β should be at least 0.2 to be considered relevant. Following guidelines from , , a model is considered statistically somewhat significant (*p) with a p-value < 0.1, quite significant (**p) with a p-value < 0.01, and highly significant (***p) with a p-value < 0.001. Direct effects analysis showed that EA significantly and positively affects DOC (β = 0.329; p < 0.001) and PRB (β = 0.524; p < 0.001), supporting hypotheses H1 and H12. In contrast, PDS exhibits a significant negative impact on DOC (β = -0.170; p < 0.01), leading us to partially reject H6 as the effect, though significant, is negative. TT also shows a positive influence on DOC (β = 0.300; p < 0.001) and a moderately positive effect on PRB (β = 0.300; p < 0.1), affirming hypotheses H3 and H10. However, the relationships between PRB and DOC (β = 0.037; p > 0.1) and between PDS and EA (β = -0.034; p > 0.1) did not reach significance, resulting in the rejection of hypotheses H2, H7, and H9.\nThe explanatory power of the model is noteworthy, with EA, TT, PDS, and PRB explaining 32.2% of the variance in DOC (R² = 0.322), and EA and TT explaining 33.4% of the variance in PRB (R² = 0.334). PDS, however, accounts for only a minimal 0.1% of the variance in EA (R² = 0.001).\nExamining the indirect effects, the analysis indicated that the pathway from PDS to DOC via EA was not significant (β = -0.011; p > 0.1), leading to the rejection of hypothesis H8. Similarly, the indirect pathway from EA to DOC via PRB was insignificant (β = 0.020; p > 0.1), resulting in the rejection of hypothesis H4. Furthermore, the mediation from TT to DOC through PRB was also not supported (β = 0.005; p > 0.1), rejecting hypotheses H5 and H11. Despite EA's direct impact on DOC, its indirect influence through PRB is unsupported, leading to the rejection of hypothesis H13.\nThese findings elucidate the complex interplay between educational awareness, trust, perceived risks, and parental influences in shaping data ownership and control among youth. Future research should expand on these findings with longitudinal data to better understand the dynamic changes and causal relationships over time."
    },
    {
      "title": "Qualitative findings",
      "content": "Building upon our previous research detailed in , this study extends the qualitative analysis of stakeholder perspectives on privacy in AI systems. We further explored the viewpoints of educators, parents, AI professionals, and young digital citizens, examining open-ended survey responses to identify common themes around privacy concerns, which include lack of awareness, data misuse, and feelings of loss of control. In line with the prior findings , Young digital citizens repeatedly expressed a limited understanding of AI data practices, highlighting significant gaps in their knowledge and awareness. For example, one young participant remarked, \"I do not know enough about them to be concerned about what I should be concerned about\" (#Participant 1). This feeling was shared by parents and educators who expressed concern about the potential dangers of uninformed data sharing in regards to children's safety. A concerned parent noted, \"Unauthorized access to personal data can harm children in ways they don't even understand yet\" (#Participant 2).\nThe misuse of data was a critical concern shared by all participants who expressed worry about how their information might be exploited. A youth participant expressed unease about corporate misuse of data, stating, \"What worries me is the companies involved selling, and storing my data for purposes other than personalizing\" (#Participant 3). An AI researcher pointed out a systemic liability, adding, \"Once data is shared, there's no going back. Misuse becomes inevitable\" (#Participant 4).\nAll three demographics expressed the feeling of powerlessness over the control of personal information being used by AI systems. One educator criticized the excessive demand for data, stating, \"AI systems ask for too much data, more than they need to serve their purpose\" (#Participant 5). Echoing this concern, a young participant reflected: \"AI collects too much personal stuff. Most of us don't know what they do with the data, which feels like losing control\" (#Participant 6).\nThese findings emphasize the challenges associated with the CPM framework concepts of boundary turbulence and privacy control. The responses highlight an urgent need for increased transparency, trust, and education to empower all stakeholders to manage their privacy more effectively in AI environments."
    },
    {
      "title": "DISCUSSION",
      "content": ""
    },
    {
      "title": "Education and Awareness Enhances Data Control and",
      "content": "Risk Perception Education and Awareness (EA) was a significant factor in enhancing the perception of Data Ownership and Control (DOC). Higher levels of awareness were associated with greater confidence in managing personal data, supported by the construct's reliability metrics (AVE = 0.553, CR = 0.711). These findings align with the principle that knowledge is fundamental in establishing privacy boundaries. Educating individuals about privacy issues gives them the tools to establish strong privacy guidelines, allowing them to manage risks and benefits more effectively. However, the absence of significant mediating effects through Perceived Risks and Benefits (PRB) suggests that theoretical understanding alone may not lead to effective privacy management. Practical training, such as workshops and interactive exercises, may help close the gap by empowering individuals to apply their knowledge in real-life situations."
    },
    {
      "title": "Parental Data Sharing (PDS) Facilitates Privacy",
      "content": "Management Parental Data Sharing (PDS) had a meaningful impact on youth's privacy management, with the study finding a significant yet negative effect on DOC (β = -0.170, p = 0.005). This suggests that overly controlling parental behavior might be undermining youth's confidence in managing their privacy. Collaboration over boundary management control, where parents guide instead of dictate, proves to be more effective in fostering healthy data-sharing practices. Interestingly, PDS did not significantly affect Education and Awareness (EA), highlighting the need for more structured family-oriented educational programs. These types of initiatives should focus on promoting mutual respect and understanding in data-sharing practices in order to reduce boundary turbulence and foster a more balanced dynamic between parents and youth."
    },
    {
      "title": "Perceived Risks and Benefits (PRB) Shape Decisions",
      "content": "Perceived Risks and Benefits (PRB) significantly influenced DOC, with youths who recognized potential risks associated with AI systems exerting more control over their personal data. This underscores the importance of risk awareness in influencing privacy behaviors, highlighted by individuals weighing the trade-offs between the benefits of these technologies and privacy concerns. Effective privacy management is supported by these risk-benefit practices, which aid in making informed data-sharing decisions. The findings suggest that integrating risk-benefit analysis into educational training could help enhance youths' ability to make informed choices. By promoting practical tools and using real-life examples, such programs can help individuals develop a more nuanced understanding about how to balance convenience and privacy risks when using AI systems."
    },
    {
      "title": "Transparency and Trust (TT) Influences Data Control",
      "content": "Transparency and Trust (TT) was found to have a significant positive effect on Data Ownership and Control (DOC). These findings highlight the importance of clear and accessible communication in privacy management. The construct's reliability metrics (AVE = 0.449, CR = 0.686) suggest room for improvement in conceptualizing TT's role in empowering individuals. Transparent practices, such as simplified disclosures and privacy summaries tailored to youths, can enhance understanding and trust, ultimately reducing boundary turbulence. While transparency helps mitigate data-sharing practices, additional tools that allow users to visualize and adjust their privacy settings in real-time could further empower them to take control over their data."
    },
    {
      "title": "Education and Awareness (EA) as the Strongest Predictor",
      "content": "of DOC among all factors EA emerged as the strongest predictor of DOC, underscoring the transformative potential of knowledge in privacy management. Youths who are well-informed about AI systems and data privacy issues demonstrate greater control over their personal data, reaffirming the role of education in enabling effective boundary regulation. The study highlights the need for comprehensive educational programs that combine theoretical learning with hands-on training. By integrating practical applications, such as interactive sessions and realworld problem-solving activities, these programs can help youths internalize privacy principles and implement them effectively in their daily lives."
    },
    {
      "title": "Theoretical Implications",
      "content": "This study advances privacy research by applying the principles of boundary regulation to the context of youth interactions with AI technologies. It highlights the dynamic interplay between education, parental influence, risk perception, and transparency in shaping privacy behaviors. The findings suggest that collaborative frameworks, where knowledge and trust are central, can reduce boundary turbulence and enhance privacy management. Policymakers and system designers should focus on creating environments that empower users with customizable tools and accessible privacy settings. For educational initiatives, integrating risk evaluation and boundary-setting concepts can provide youths with a stronger foundation for managing their privacy in AIdriven ecosystems."
    },
    {
      "title": "Limitations and Future Works",
      "content": "Our study faces some limitations that affect the generalizability and depth of the findings. The use of purposive sampling and a focus on three distinct stakeholder groups, young digital citizens, parents/educators, and AI professionals, may not fully represent the broader population, limiting the generalizability to other demographic groups such as nontechnical users or policymakers. Additionally, the crosssectional design only offers a snapshot in time, constraining our ability to make causal inferences and capture the evolving nature of privacy attitudes and behaviors. Future research could benefit from employing random sampling methods and longitudinal designs to validate these results across a more diverse range of populations. Additionally, the moderate reliability of some constructs, particularly Trust and Transparency (TT) with an AVE of 0.449 and a CR of 0.686, suggests a need for methodological refinement to more accurately capture the complexities of privacy management behaviors within AI contexts. This approach would strengthen the results and help provide a more in-depth comprehension of the societal applications of CPM concepts."
    },
    {
      "title": "CONCLUSION",
      "content": "This study applied CPM theory to examine the privacy perspectives of young digital citizens, parents/educators, and AI developers/researchers using five key validated constructs: Data Ownership and Control (DOC), Parental Data Sharing (PDS), Perceived Risks and Benefits (PRB), Transparency and Trust (TT), and Education and Awareness (EA). Data was collected using survey instruments, refined with a pilot study, and analyzed using Partial Least Squares Structural Equation Modeling (PLS-SEM). The resulting model shows that EA significantly influences DOC and PRB, underscoring the critical importance of privacy education in facilitating more effective boundary management strategies. Similarly, TT positively influences DOC, reinforcing the role of transparency in building trust and reducing boundary turbulence. However, the minimal mediation of PRB and the negative effect of PDS on DOC underscore the complexity of privacy behaviors, suggesting external factors like parental dynamics and usability concerns may play critical roles. These results emphasize the need for user-centric privacy controls, tailored transparency mechanisms, and collaborative educational initiatives to empower stakeholders and reduce privacy risks. While this research advances the application of CPM Theory to AI contexts, future studies should expand demographic diversity, refine constructs for improved reliability, and explore longitudinal shifts in privacy behaviors to further inform privacy-centric AI system designs."
    }
  ]
}