{
  "title": "The potential for co-operatives to mitigate AI ethics catastrophes: perspectives from media analysis",
  "sections": [
    {
      "title": "Abstract",
      "content": "Would the world have seen less AI-related scandals if more AI companies operated as co-operatives? As a response to multiple high profile tech scandals within the last decade, there has been an increased call for introducing more accountability in the AI industry. However, it is unclear to what degree the proposed efforts have been or will be effective in practice. The question remains whether these incremental, multi-stakeholder AI ethics efforts are in fact trying to address a fundamentally systemic issue inherent to the existing corporate power structure. As an attempt to address this question, we gain an understanding of the major themes in high profile AI-related catastrophes in the last four years (2018-2021) through an inductive media analysis. We then investigate how the principle of democratic governance and distributive executive power -core to co-operative organization structure -could have prevented or mitigated the contributing factors of the reported events. We find that the vast majority (71%) of the recent AI ethics scandals are not the result of a lack of knowledge or tools, but attributed to power dynamics that hinder the ability of internal stakeholders from taking action. We present the co-operative governance structure as a possible mitigating solution to addressing future AI ethics catastrophes, and provide a critical look at practical challenges inherent to AI co-operatives."
    },
    {
      "title": "INTRODUCTION",
      "content": "With the increased frequency and public awareness of AI ethics related incidents in the last decade, many multistakeholder initiatives have produced AI ethics principles , toolkits, and guidelines . Despite the promise of AI ethics to have tangible real world consequences, the efficacy of these new interventions in practice remain in doubt , . Some argue that ethics guidelines are ineffective because of their deontological framing of ethics as a checklist, since a checklist offers a series of potentially superficial maxims that may ignore deeper underlying ethical factors . Others warn that ethical guidelines may only be effective in placating public concerns without binding the company to real ethical action, which can lead to a \"boiling frog problem\" where practices can be incrementally changed or eroded over time .\nHagendorff posits that a possible reason for shortcomings in AI ethics is due to corporate environments themselves: ethics processes and standards may be in some ways antithetical to environments where those in power favor speed and profit maximization . Boddington echoes a similar view, suggesting that ethical decision making -especially in a field as competitive and rapidly growing as AI -is treated as either an afterthought, or a public relations exercise .\nA common issue in these critiques is that the values of those ultimately in power do not correspond to the values of those who must interact with AI. A co-operative (\"co-op\") is a readily available business model that has the potential to empower those who interface with the company's AI products, thereby reducing the risk of harm. In a co-op, the owners of the business are the members (potentially workers, users, or other stakeholders), whereas in a traditional company, the owners of the business are the shareholders who often have little to do with the day-to-day operations of the company. Further, a co-operative company is more democratic than a traditional one because each owner has one vote, whereas in traditional companies, voting power can be proportional to shares.\nThere has been much work on \"democratizing AI\", though \"democratization\" has multiple meanings in this context. Garvey points out that many major tech companies \"democratize\" their AI by releasing APIs and libraries for free, but it is difficult to pinpoint how such initiatives differs from earlier open access movements . Additionally, this approach is much different from, say, democratizing the workplace itself. Much prior work has focused on democratizing education and transparency in AI , , while others emphasize facilitating \"communication channels between experts and society\", enabling contributions from the general public to be incorporated into the research life cycle . These approaches are extremely important and a crucial step, though this still leaves open issues pertaining to the democratic governance of AI. Frameworks for democratizing AI have been proposed, and offer a promising starting point for democratic AI governance . The co-operative model is an \"out of the box\" method to create democratic organizations. They already exist as legal entities in many countries, and engender democratic AI principles, highlighted by Garvey .\nIn this article, we conduct a media analysis of AI ethics scandals to get an understanding of the most popular public AI ethics issues. (section II) Not all AI ethics issues are equal, and they all vary in importance in the public eye. Our review is by no means an objective survey of the AI ethics landscape, but instead it gives insight to a populist perception 978-1-6654-5713-2/23/$31.00 ©2023 IEEE of the most pressing topics. We provide a content analysis of these scandals as a means to discover major overarching categories and themes (section III). Building on our themes, we provide an analysis of the ways co-ops can (and cannot) address future high profile AI ethics failures (section IV). We conclude by highlighting the significant number of AI ethics issues that can be mitigated by a more symmetric power structure in organizations (section VI) , and outline some of the shortcomings of the co-op model (section V).\nA co-operative is a democratically owned, value driven business. They are legally recognized corporate entities in jurisdictions all over the world.\nThe first formal co-op business, named the Rochdale Society of Equitable Pioneers, was created in 1844 at the tail end of the industrial revolution in Rochdale, England . The aim of the company was to pool the purchasing power of consumers to buy high quality food which they otherwise couldn't afford or source. This company was owned by the consumers, and was thus called a consumer co-operative. Today, there are many different kinds of co-ops, such as: worker co-ops (companies owned by the workers), financial co-ops (credit unions owned by customers), and multi-stakeholder co-ops (companies that are owned by some combination of owner classes), among many others .\nThe Rochdale Society established seven principles that coops should abide by. The International Co-operative Alliance describes them as :\nVoluntary and Open Membership Membership is noncompulsory and open to all persons regardless of race, gender, etc. 2) Democratic Member Control One member is one vote.\nThe board must be majority members. 3) Member Economic Participation Members democratically control the capital of the organization. 4) Autonomy and Independence When entering agreements with other companies they must maintain democratic control of their own actions. 5) Education, Training, and Information Co-ops must invest in the education and training of members and allow information to pass freely. 6) Cooperation among Cooperatives Co-ops help their own sector of co-operatives. 7) Concern for Community A focus on sustainable community building. The members of a co-op are also the owners . A coop's board of directors typically consist of members, whereas in a traditional company, the board of directors would typically consist of shareholders. In a co-op, one member has one vote, whereas in a traditional company an individual's voting power would be proportional to their ownership . A democratic governance structure, and some form of member economic control are typically a part of a co-op's articles of incorporation or bylaws. The degree to which these principles are legally enforceable vary from country to country. In Canada, democratic governance and member participation is legally required under the Canada Cooperatives Act .\nIt is a common misconception that co-ops must be nonprofit organizations, when in reality, most co-operatives are for-profit enterprises. Co-ops can scale to be billion dollar companies 1 . Some large-scale examples of co-ops include: Desjardins Group (Canada), Dairy Farmers of America (USA), Mondragon Corporation (Spain), and Crédit Agricole (France). While co-ops are underrepresented in tech, many tech co-ops do exist 2 ."
    },
    {
      "title": "METHODS",
      "content": "To gain a understanding of the thematic landscape of recent failures in AI ethics, we conducted a media analysis of news articles from 2018-2021. A media analysis doesn't offer a completely objective look at the AI ethics landscape. Rather, it paints a picture of pop culture. If something is frequently reported on, it doesn't necessarily mean it's a \"larger ethical problem\"-it simply means it is a more popular story. We chose the news article as our unit of analysis, as we wanted to frame our co-operative analysis in terms of what the most high profile AI ethics issues are in popular culture from 2018-2021.\nWe started our media analysis by searching keywords (\"AI\" OR \"artificial intelligence\") AND (\"ethics\" OR \"scandal\") in news.google.com while in Incognito Mode. Articles that were not explicitly about AI were discarded (e.g. scandals in tech hiring practices). We looked at the first 5 relevant articles for each year, and used them as a basis to find additional related articles. In total, 49 articles were selected for analysis. To be transparent about possible political and other biases that may be present in different news outlets, we provide a breakdown of the news sources in our corpus in Figure 1a.\nWe used content analysis, an inductive qualitative research method to discover overarching themes and categories from news articles , . Content analysis can be used as a \"bottom up approach\" to systematically describe the meaning of qualitative data without a prior theory or framework to guide understanding . We specifically followed the content analysis procedure described by in Erlingsson and Brysiewicz . The procedure is as follows: first, a researcher coded the content of the articles according to its literal meaning. The codes were then clustered by the researcher into categories based off their semantic relatedness. In content analysis, a category is intended to describe manifest meaning, and answer the \"what\", \"where\", \"who\", and \"when\" of groups of codes . Finally, categories inductively grouped into themes based off a large overarching explanatory connections. Themes describe the \"why\" and \"how\" of our data, and tend to describe describe more latent meaning . A single article can be associated with multiple categories and themes.\nThe content analysis yielded 12 categories and 3 themes (table I), which are reported in section III. The complete, coded dataset is available online 3 . Based on the themes and categories, we relate the issues surrounding major AI ethics scandals with the co-operative model and its principles. We conducted a critical analysis of the ways that the co-op model could be either helpful or irrelevant in addressing the common points of failure.\nIn this process, we acknowledge that all AI ethics issues are also systemic issues. No article in our corpus reported a scandal that could be decoupled from its systemic context. Thus it is impossible isolate a \"purely AI\" problem from a given scandal, as every AI ethics problem is a product of its intersectional environment."
    },
    {
      "title": "CONTENT ANALYSIS",
      "content": "We identified three themes through our content analysis. First, the majority of the articles reported AI ethics scandals as stemming from problematic power dynamics -such as asymmetric power structures within organizations and lack of transparent/open stakeholder discourse (subsection III-A). Second, a number of articles mentioned mechanisms stakeholders could use using to inspire action toward normative goals (subsection III-B). These include calls for regulatory interventions, critical dialogue about the efficacy of corporate ethics boards, and reducing of AI knowledge gap between stakeholder groups. Lastly, our corpus also included articles that focused on the consequences of AI failures, such as fostering extremism, and the discrimination of marginalized groups (subsection III-C).\nIn this section, we describe each cluster of topics as categories within the three themes. An overview of all themes and categories are outlined in table I. The number of articles that are mapped to each category are illustrated in Figure 1b."
    },
    {
      "title": "Theme 1: Power Dynamics Affect Ethical Abilities",
      "content": "Out of the 49 news articles, 35 (71%) thematically touched on issues of power dynamics. In these articles, alternative actions toward better outcomes were identified or known to the stakeholders involved, but they were not empowered to take these alternative actions. For example, employees who do not have sufficient autonomy are not allowed to be critical out of fear of retaliation, or risk being fired. This could stem from many reasons: the news articles revealed 6 unique categories that broadly contribute to this: a lack of democratic power, criticism, responsibility, toxic culture, transparency, and conflicts of interest. These are each elaborated upon in turn.\nCategory 1.A: Democratic Power: Nineteen articles addressed issues of democratic power. We characterize democratic power in AI as the degree to which stakeholders have the power to influence decisions that affect their own needs and desires. Illustrative articles include an Equal Times report of how between 2009-2018 a Dutch government fraud detection system wrongly accused 26,000 parents of committing childcare benefit fraud . Many of the accused had no option of recourse and ended up losing their homes, going into debt, and divorce .\nSome articles highlight that ethical issues have successfully been petitioned through employee walkouts. The New York Times reported that \"the most significant [ethical] changes have been driven by employee protests\" . Others highlight that exerting democratic power through such bottom-up movements is not always effective. For instance, Castelvecchi reports that over 1,400 US mathematicians wrote an open letter to colleagues imploring them to stop creating predictive policing algorithms due to concerns about systemic racism, and the ultimate efficacy of this petition remain unknown . CNBC reported that the Center for Democracy & Technology petitioned Facebook for more control in how ads are administered due to targeting algorithm reinforcing racist and sexist biases, and Facebook acknowledged and committed to fixing the problem. However, a follow up investigation two years later revealed that nothing had ultimately changed about their advertising algorithm . In addition, Johnson points out that large companies often deploy anti-collectivist tactics . In their article, they describe how Google infamously fired employees in retaliation to unionization efforts .\nCategory 1.B: Criticism: Five news articles contained issues pertaining to criticism. This category pertains to the degree to which stakeholders are free to voice their critique of an AI project's direction or decisions by those in power. A lack of critical dialogue can be detrimental to model evaluation, exacerbating pre-existing ethical issues. In the case of Timnit Gebru, it is understood that an academic article she coauthored which was critical of large language models was a factor in her dismissal due to it implicating her employer . Haven from MIT Technology Review states that the effectiveness of algorithmic policing technology remains uncertain simply because police departments have dismissed audits of the systems from both outside and within governments . Director of policy research at the AI Now Institute Rashida Richardson said that \"police are able to go full speed into buying tech without knowing what they're using, not investing time to ensure that it can be used safely...and then there's no ongoing audit or analysis to determine if it's even working\" . Furthermore, a CNBC article by highlights that critical feedback of AI systems must come from humans, not machines. The article quotes policy analyst Natasha Duarte saying \"you can't just build an algorithm to police the algorithm\", noting that there are human situations that cannot be resolved algorithmically .\nCategory 1.C: Responsibility: Seven of the articles highlighted issues of responsibility. This category encompasses how responsibility may be diffused through an organization, where no one person feels solely responsible for the risks incurred by an algorithm. It also includes how responsibility can be evaded, or deferred.\nFor instance, Hao from MIT Technology Review reports that Joaquin Quiñonero Candela -director of applied machine learning at Facebook -considers the job of tamping down misinformation and extremism as the job of other teams. Yet, the article concludes that no particular team has been mandated to work on this task . Further still there is a large question of who can be held accountable for AI ethics failures. For instance, in discussing the case of AI use in building autonomous weapons, a Forbes article asserts that details of who should build targeting algorithms and who should be held responsible for resulting failures remain unaddressed . Sometimes responsibility may be superficially taken, but ultimately be non-binding or ineffective. Arvanitakis critiqued large organizations for signing up for ethics initiatives that are not enforced or enforceable, and as a result have not altered any practice . Hao echoes this sentiment by reporting that while fairness guidelines exist at Facebook, they are not enforced, and testing algorithms for specific fairness metrics is optional .\nResponsibility for certain ethical issues may also be deferred. In the case of an Uber self-driving car killing a pedestrian, the MIT Technology Review reported that some responsibility was deferred to the car's safety co-pilot, not the autonomous algorithm .\nCategory 1.D: Toxic Culture: Five articles outline how toxic working environments within technology companies can suppress voices of the marginalized and hinder ethical action by stakeholders within the company , . This also leads to both the gatekeeping and attrition of valuable employees. For example, the India Times reported that the mistreatment of Google's AI team demoralized workers, sparking an exodus of employees. Google engineer David Baker said that Gebru's mistreatment \"extinguished\" his will to work in the company . Gebru has stated: \"if you look at who is getting prominence and paid to make decisions [about AI design and ethics] it is not black women\" . The Financial Times reported that this problem is endemic to Silicon Valley, and is expected to last for years .\nCategory 1.E: Transparency: Transparency is the second most frequently occurring category that emerged from content analysis. A lack of transparency makes it harder to hold organizations and people accountable, and can be used to centralize power. MIT Technology Review reported that there is a lack of transparency within organizations: Forbes highlights that it is typical for a companies organizational structure to be shrouded in mystery . Many times it is unknown how tech companies will use an individual's data-whether it will be weaponized, or guarded . Organizations also offer little transparency into how AI systems are built, making it difficult for 3rd parties to evaluate them .\nAn important aspect of AI systems is their explainability. Explainable, transparent AI ensures that they are also auditable . Unexplainable AI is hard to monitor or scrutinize when decisions are wrong . A lack of transparency hurts everyone, even people in positions of power. Infamously, the Softbank backed startup Engineer.ai managed to secure nearly $30 million of series A funding by pitching the concept of algorithmic app development. The only catch, as the Wall Street Journal reports, is that there was no transparent description of the AI involved. In the end, the startup did not develop apps algorithmically, but instead through human labor in India . The Verge opines that there is a mysticism about AI, and that this mysticism can be exploited \"to dupe both the public and even investors into believing a technology is more sophisticated than it really is\" ."
    },
    {
      "title": "Category 1.F: Conflicts of Interest",
      "content": "There is an intrinsic conflict of interest between the values of a traditional company, and the values of a human. A publicly traded company must grow, and this imperative don't always translate into effective practice in AI. Former Facebook employees remarked that it could be difficult to get company support if their work didn't directly improve Facebook's growth . Facebook's Society and AI Lab (SAIL), created to examine the societal effects of its algorithms, routinely proposed \"ideas antithetical to growth\" leaving many ideas to stay largely academic . Ideas that are adopted, such as AI principles, have been seen in the case of Google to be largely open to interpretation and \"overseen by executives who must also protect the company's financial interests\" . The U.S. Consumer Financial Protection Bureau has received pushback to their proposed antibias regulation by financial and insurance companies because they claim it puts them at a competitive disadvantage . In this manner, both government and internal company efforts are stifled by growth based financial values.\nThere is also much controversy of the role of corporate influence in research. Venture Beat notes that many labs are dependent on corporate funding. UC Berkley Associate Professor Rediet Abebe is quoted saying \"If a bunch of us are taking money from the same source, there's going to be a communal shift toward work that is serving that funding institution\" ."
    },
    {
      "title": "Theme 2: Mechanisms for Actionable Change",
      "content": "This theme encompasses approaches stakeholders have taken to tackle AI ethics issues, and calls to action by advocates of AI ethics.\nCategory 2.A: Government: Enacting change through the government, be it through regulation or new laws, was a present category in 14 of the news articles. The government can be ideal to make ethics principles \"binding\" in a way that company policy cannot be.\nThe Verge quotes TU Delft professor Ben Wagner saying \"most of the ethics principles developed now lack any institutional framework...they're non-binding. This makes it very easy for companies to look [at ethical issues] and go, 'That's important,' but continue with whatever it is they were doing beforehand\" . Some steps to regulation have already been taken in some constituents-New York City has introduced an algorithm accountability bill to address discrimination . However, regulation merely existing doesn't address all the issues. The EU's General Data Protection Regulation is focused on privacy and automated decision making, but nothing beyond this . Anti-trust hearings have been conducted on large tech companies. Of particular interest is preventing a \"data monopoly\". Some of the more extreme proponents of regulation call for an outright ban on certain AI applications. For example, arguments have been made for a ban on predictive policing systems because they violate the presumption of innocence .\nCategory 2.B: Self-Regulation: This category pertains to the use of internal company instruments such as ethics boards or codes of ethics to guide ethical decision making in a company. This category was present in 16 news articles.\nThe Verge describes ethics boards as being \"in vogue\" . Many large companies like Google, Amazon, and Microsoft have some form of an ethics committee . Moreau has claimed that ethics boards should be independent to work . When companies have internal ethics boards, they are riddled with transparency issues. Microsoft claimed that they lost \"significant sales\" due to their AI ethics committee recommendations, but it is unknown what ethical guidance they've accepted or dismissed and why . The overall effectiveness of the ethics board itself has been called into question, as some of them appear to be poorly designed . Vox argued \"a role on Google's AI board was an unpaid, toothless position that cannot possibly, in four meetings over the course of a year, arrive at a clear understanding of everything Google is doing, let alone offer nuanced guidance on it.\"\nAnother mechanism organizations may employ to self govern are ethical principles or guidelines. This is also in vogue, as AI Now has claimed there has been a \"rush to adpot\" ethical codes in recent years . IBM, Microsoft, and Google have all published their own ethical principles . Some see ethics principles as a soft guide for future AI legislation , while others see them as a way to \"preempt some of the damage\" by using ethical AI design principles and understand ethical issues a priori . In a longform feature, Venture Beat presents the opinion that self-regulation at big tech companies simply isn't possible, and that companies participate in selfregulation to avoid actual regulation. Why Not Lab director and Global Partnership on AI steering member Christina Colclough is quoted saying \"a lot of governments have let this self-regulation take place because it got them off the hook, because they are being lobbied big-time by Big Tech and they don't want to take responsibility for putting new types of regulation in place\" .\nCategory 2.C: Knowledge: A lack of education about AI, ethics, and their interrelations can lead to poor decision making. This can be realized in many stakeholders: those who use AI products will make bad policy decisions if not properly informed with how AI works in the first place. Those who create AI may also design harmful products if they are not aware of their application domain. This category was present in 10 articles. Activists have pushed for increased education around AI ethics for both programmers and laypeople . It is argued to be vital to democratize AI education to understand how one's unconscious bias affects the design of AI systems . AI education in general is also important for businesses and organizations. ItProPortal claims that \"most businesses in the UK do not have a solid understanding of AI\" and that \"AI has become a buzzword and not a reality for British businesses\" . The same can be said of government institutions like the police . The case of Engineer.ai, which duped investors out of millions of dollars by lying about their AI capabilities, was also possible due to poor general AI knowledge . Three different articles called for epistemic re-ogranization. CNBC presented the opinion that the engineers designing algorithms should also be trained in non-technical areas . Venture Beat highlighted voices that advocated for the use of interdisciplinary AI research teams, bringing researchers as diverse as sociologists to oceanographers on board ."
    },
    {
      "title": "Theme 3: Consequences of Failure",
      "content": "Though all themes in this analysis are related to failures in AI ethics, this theme broadly encompasses the consequences of these failures and who is affected by them. The ultimate consequences of failure are numerous, though in our corpus, 3 were ultimately discussed: issues effecting fairness, extremism, and reinforcing pseudoscience.\nCategory 3.A: Fairness: Failures in AI ethics can affect the fairness of algorithms, exacerbating bias and oppressive forces in society. Fairness was the most frequently occurring category, and was present in 23 articles.\nReuters reported of an Amazon AI recruiting tool that was biased against women-penalizing applicants who attended all women's colleges and resumes that contained the word \"women's\" . An investigation by the National Bureau of Economic Research found that lending algorithms are also discriminatory against race . Facial recognition software has been routinely shown to have racist and sexist biases as their accuracy on identifying white men is disproportionately higher than any other race or gender , . Dutch tax fraud algorithms had disproportionately higher false positives for families with foreign sounding names and poor backgrounds .\nBeyond amplifying unconscious bias, articles described how AI may be intentionally used for oppression. The Financial Times documented \"Uyghur detection\" algorithms that have been used by the CCP to detain and surveil Uyghurs in Xinjiang . The U.K. firm Cambridge Analytica was infamously linked to influencing the 2016 U.S. presidential election through misinformation and psychographic profiling . Amazon uses computer vision to monitor the productivity of workers and algorithmically fire them for underworking , .\nCategory 3.B: extremism: Five articles mentioned how failure can spread extremism, specifically on social media. Facebook, in an official blog post, has taken responsibility for a New York Times broken story where the social media platform aided the Myanmar genocide through negligent content moderation , . CNet reported that Facebook currently faces a lawsuit for not removing anti-muslim hate speech, with armed anti-Muslim protests even being organized on the platform . USA Today highlighted how extremist conspiracy theories such as those pertaining to QAnon and COVID are further perpetuated through content recommendation sites like Facebook and TikTok .\nCategory 3.C: pseudoscience: This was the least frequently occurring category, occuring in only 4 news articleshowever, it should be noted that the frequency that a category appeared in our corpus doesn't denote its importance. AI has the veneer of being \"scientific\" and can be wrongly used to propagate and reinforce pseudoscientific beliefs.\nProlific cases include \"physiognomist AI\", where in 2016 and 2020 two studies were released that claimed to link facial features to criminality, one of which boasted \"80% accuracy\" . Forbes reported on voice scraping algorithms that made connections between physical aspects of speech and job performance-a connection which likely has little explanatory power . An extreme case discussed (that is as of now only hypothetical, and has not been attempted) is using candidate DNA to predict job success .\nMIT Technology Review highlighted that all machine learning algorithms infer from correlations, and that people often make the fallacy of assuming causation from the model's correlations .\nMuch psuedoscience attempts to prove normative claims with unsound methods, and has a history of supporting those in power and oppressing marginalized groups. AI that embraces these notions is equally oppressive."
    },
    {
      "title": "CO-OPERATIVE PRINCIPLE ARGUMENTATION",
      "content": "In the following section, we evaluate the degree to which cooperative principles help to address the ethical issues that emerged from content analysis. A high-level overview of the results is illustrated in Figure 3."
    },
    {
      "title": "On co-operative principle 2: democratic member control",
      "content": "This principle is legally intrinsic to the co-op's articles of incorporation. The executive decisions of the co-op is who supply facial detection algorithms with data also were owners, they would be able to advocate for their needs in a way that would be consistently heard by the company. We could also imagine a co-operative where the owners are workers.\nIn cases where employees petitioned companies without any meaningful results, having those that petition the company actually own the company itself would ensure that their needs are translated into action. This also assists with the closely related Category 1.B Criticism. In traditional companies, it could jeopardize a worker's career to speak out. In a co-op, if a member has been unjustly fired, and other members believed it was unjust, they may be able to pass a special resolution in a general meeting to rectify the situation. Any business decision such as this could also be democratically appealed in general meetings. This gives workers more security over their careers. Since any member of the co-operative may vote on issues important to them, or bring issues to the forefront to a vote in a general meeting, this in essence flattens the hierarchy, empowering workers to be more proactive in the running of the company. In such an environment, criticism is able to flow more freely than in a traditional company.\nCategory 2.B, Self-Regulation found that self-regulation suffers mainly from having no internal accountability. If the company was democratically run, and ethics was an issue that the members cared about, then companies would be held accountable by the members themselves. If some internal mechanism such as an ethics board, or code of ethics, was not sufficiently addressing member concerns, they would be able to democratically create systemic change within the address collective goals.\nThis principle has only partial relevance to Category 1c: Responsibility. Since democratic control provides greater internal accountability, situations where ethics initiatives are not formally enforced are mitigated. However, when it comes to the reported problems of diffusion of responsibility, or deferred responsibilities, the principles and organizational structure of a co-op does not offer any apparent benefits compared to a traditional company."
    },
    {
      "title": "On co-operative principle 3: member economic participation",
      "content": "Member economic participation enables a \"cooperative economy\" , resulting in more equal pay, an egalitarian workplace, and thus balances power relations and lessens the degree of capitalist exploitation. This offers an interesting point of discussion pertaining to Category 1F: Conflicts of Interest. In this category, we frequently observed cases where companies put finances over ethics. In a co-operative, all members have a vested interest in the finances of the company-annual surplus is often paid out to members in the form of dividends or patronage. In this regard it is unclear if members will put finances above ethics as well. This seems to be a topic very dependent on the culture of the members. While it has been argued in the past that co-op members run the risk of becoming capitalists themselves ,\ncase studies of functional co-operatives contradict this and the \"neoclassical economic understanding of humans as selfcentered utility maximizers\" ."
    },
    {
      "title": "On co-operative principle 5: education, training, and information",
      "content": "This principle has relevance to Category 1.E: Transparency, and Category 2.C, Knowledge.\nOn transparency, this principle requires the free flow of information within the co-operative. Most internal company data must be accessible to members. This could include anything ranging from the company's balance sheets, to source code. If knowledge is not sufficiently transparent or available, members could vote to rectify the situation. This reduces the centralization of power through withholding information. It ensures that members, if desired, could have an understanding of how user data is being used, and how AI systems are being architected. This principle does not offer much in terms of explainable AI since it is only an organizational value. It also does not necessarily solve the issue of organizational structure being \"shrouded in mystery\" to non-members. The co-operative has an obligation for free flowing information to its members, but not the general public. This would be a decision the members would make.\nOn Category 2.C, Knowledge, co-ops have a vested interest in educating its members. This can help de-mystify AI to people internal to the company. As content analysis revealed, when people are better informed about AI, they can make better decisions. This co-operative principle does not necessarily help with education outside of the company, however. The current state of outreach activity, or education to private individuals or government, would not necessarily change due to co-ops since it is not an obligation. That said, many co-ops do participate in outreach activity, as frequently seen in local credit unions. Additionally, co-operatives in Italy are required by law to invest 3% of their surplus into other co-operatives, many of which are social enterprises .\nThere is a chance that with open and free education, this may assist with Category 3.F, Pseudoscience. At its best, pseudoscientific methods can simply stem from being uninformed. At its worst, pseudoscientific methods are intentionally used to reinforce power structures and prior biases of the researcher. If pseudoscientific practice merely stems from ignorance, then there is potential for this to be resolved through open education within the co-operative. Yet if the coop is small, or has the wrong demographic, then diverse or adequate education may not reach its members. It's for this reason that the co-operative structure only partially, but not fully, addresses this issue."
    },
    {
      "title": "Other principles and discussion",
      "content": "There are a number of other principles that have applications to specific circumstances. Principle 4, Autonomy and Independence assists with aspects of Category 1.F, Conflicts of Interest-if members uphold this principle, then it may restrict external companies from exerting control that may compromise the co-ops values. Principle 7, Concern for Community helps with Category 1.B, Criticality. Community engagement can help designers realize their own biases, and give them a hands-on understanding of the societal effects of their work.\nThe co-operative model was not very applicable to category 2A, Government. Co-ops don't have much of an advantage for inciting government change compared to a traditional corporation. Both traditional and co-operative businesses are capable of lobbying the government. One major difference is that if a co-op were to lobby the government, it would likely be in the best interests of the members, and not the external shareholders.\nIt is also not directly shown whether or not co-ops help in preventing a toxic workplace culture (Category 1D, Toxic Culture). Studies on co-ops and workplace culture often suffer from selection bias-people who are more equitably minded will tend to form co-ops, so it can be difficult to say how much workplace equity is attributable to structure or the people. The one benefit co-ops clearly offer over traditional companies is that members are more empowered to speak up against unfair treatment, which shows promise for greater workplace equity . While women on average fare better in worker owned businesses, they are still subject to the same systemic issues present in the greater society at large . Likewise, a co-op is subject to the same systemic issues as any other organization, such an issue can't be solved purely by organizational structure. A study comparing co-op housing communities and traditional gated communities found that while racist housing policies may be more explicit in gated communities, there was sentiment among residents in co-op housing of implicit racism . It is for these reasons that the co-op model can be said to only partially support the category of Toxic Culture.\nThe co-op model also does not have much baring on Theme 3, Consequences of Failure. Co-ops are certainly more fair and equitable than a traditional company. Yet if a failure of AI ethics occurs under the co-operative, the consequences are no different than in traditional company."
    },
    {
      "title": "SHORTCOMINGS OF THE CO-OPERATIVE MODEL",
      "content": "It should be noted that we are not presenting the co-operative model as the one solution to the problems identified-rather, as an already available business model that could to mitigate these issues. We also do not mean to present co-operative model as a perfect solution, as it can also fail. For example, Co-op Refinery Complex (CRC), a Canadian oil refinery co-op, recently came under fire for union busting and benefit/wage rollbacks, resulting in a dramatic escalation from union members with blockades, arrests, and even bomb threats .\nA democratic governance structure also doesn't guarantee that the company itself will be democratic. It is still subject to the same internal corruption and manipulation as any other human organization. Mountain Equipment Company (formerly Mountain Equipment Co-op), was a Canadian outdoors coop which was undemocratically sold to an American private equity company. This was a internal deal, hidden from the majority of members, and was orchestrated by a corrupt board. The sale saw the company demutualize (become a traditional company), thereby stripping members of their ownership , .\nAn oft mentioned criticism in co-operative structure is of speed. Democratic process does indeed incur additional time costs. However, we argue that what companies may lose in terms of speed, they may gain from the long term well-being of the company and the end-users of their products."
    },
    {
      "title": "DISCUSSION",
      "content": "The results of our analysis suggest that co-op models can directly assist in addressing four, and partially help to address five of the twelve categories of issues related to AI ethics scandals in our corpus (Figure 3). It is also clear from the content analysis that it is impossible to decouple AI ethics issues from the greater systemic forces that create it. The coop model provides a framework to foster a more democratic power structure for the design of AI systems. Of note is at no point does the co-op model mandate specific ethics requirements-ethical behaviour is simply a potential side effect that results from distributing power and risk to all stakeholders of the company.\nOne benefit of co-operatives is its legal implications. Since democratic governance is a core part of the company's articles of incorporation, this gives members \"institutional whistleblower protection\". Members are legally empowered to hold the company to certain principles. If the co-operative appears to be violating Principle 3, Democratic Member Control, a member could escalate anywhere from passing a resolution, to mediation, to litigation. The details as to what co-operative principles are \"legally binding\" and how they are implemented varies by jurisdiction, and also within the company's governing documents. Though the principles themselves are not \"laws of the co-operative\", the fact that that the cooperative organizational structure is ratified by by law gives them an advantage over other purely principle based approaches to ethics such as codes of ethics, where no one person has effectively any institutional whistleblower protection. Adopting the co-operative model also does not require additional legislation from the government (though does not absolve the need of regulation). Another advantage co-ops have over a code of ethics is that instead of prescribing domain specific virtues or deontological rules, the cooperative structure incorporates members who have an active stake in their work, and will be better equipped to critically engage with the unique circumstances of novel ethical issues. Many times, this results in more ethical practice.\nWhat type of cooperative should AI companies adopt? We propose a multi-stakeholder cooperative, though company circumstances may affect this. A multi-stakeholder AI cooperative should include both workers, users, and potentially data producers. Workers, especially those trained in AI ethics, would be empowered to make actionable change without recourse. users have valuable on the ground insight to the impacts of the AI system. An idealist version of this co-op would also include data producers-this includes all people who provide the algorithm with data to both analyze and learn from.\nCo-operatives will not \"solve\" AI ethics, they are not infallible organizations, and on their own do not do much to address the larger systemic issues at play with regards to ethical issues surrounding AI. However, by distributing power to those who have a stake in AI development, and to those who directly suffer the risks of AI failure, co-ops can help make meaningful and actionable progress in realizing an ethical AI practice."
    }
  ]
}