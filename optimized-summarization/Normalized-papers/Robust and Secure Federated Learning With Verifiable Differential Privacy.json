{
  "title": "Robust and Secure Federated Learning With Verifiable Differential Privacy",
  "sections": [
    {
      "title": "Abstract",
      "content": "Federated Learning (FL) has emerged as a popular paradigm for training machine learning models on data from multiple sources without the need for data to leave their respective locations. Although FL is widely used, recent research indicates its vulnerability to data privacy breaches and Byzantine attacks. Addressing both threats simultaneously remains challenging, as previous defense mechanisms struggle to balance Byzantine resilience, data privacy, and training accuracy. Although combining Differential Privacy (DP) with Byzantine-resilience in FL offers potential solutions, it presents challenges due to theoretical incompatibilities that can impact the performance of large-scale models. Addressing this underexplored challenge, we propose a novel approach that enhances privacy and Byzantine-resilient in FL while maintaining training accuracy. Our method integrates DP with Byzantine-resilience techniques, overcoming the limitations of existing methods that often sacrifice accuracy or even Byzantine resistance. Our experiments carefully analyze the impact of DP noise on training accuracy and Byzantine-resilience, guiding appropriate parameter selection. The experimental results demonstrate that our approach achieves superior global model accuracy compared"
    },
    {
      "title": "INTRODUCTION",
      "content": "a S AN emerging distributed machine learning paradigm, fed- erated learning (FL) has garnered significant attention from both academia and industry. In FL, users independently train their own local models using their private data, and subsequently submit these local models to a central server. Next, the server aggregates these local models to form a new global model, which is then sent back to the users for further local training. This iterative process continues until the global model reaches convergence . Importantly, throughout this process, the users are not required to share their private data with the central server.\nHowever, recent studies have highlighted several privacy and security threats faced by FL, including gradient-based data inference attacks , , , , model poisoning and backdoor attacks , , , , . First, adversaries can exploit membership inference attacks (MIA) to infer sensitive local information, thereby compromising data privacy , . Second, the inherently non-independent and identically distributed (non-IID) nature of local datasets in FL often leads to significant bias variations in local models. Given that these biased local models directly influence the global model, malicious actors may leverage this vulnerability to launch Byzantine attacks, such as poisoning the global model , or injecting backdoors into it .\nTo address privacy leakage, prior works , , , , have focused on implementing secure aggregation (SA) algorithms using secure multiparty computation (MPC) techniques to preserve the privacy of local models and ensure lossless aggregation; however, continuous exposure to global models still poses risks of local privacy leakage , , , . Thus, there is a critical need to preserve the global model's privacy throughout each round, employing methods like Homomorphic Encryption (HE) , , and Differential Privacy (DP) , , , , , , . Although promising, HE remains nascent when it comes to preserving the global model. DP, in particular, is gaining favor due to its ability to add noise to local updates, making it challenging for adversaries to extract sensitive information. Despite its effectiveness, DP's noise injection can potentially reduce model accuracy.\nThe combination of DP and SA offers a promising approach to balancing model utility and privacy, as demonstrated in recent works , , .\nIn parallel, defenses against Byzantine attacks have primarily focused on Byzantine-resilient (BR) methods, which aim to tolerate and mitigate the impact of maliciously altered model updates , , . To simultaneously address the threats of Byzantine attacks and privacy inference attacks in FL, many works combine BR methods with privacy protection methods , , . As mentioned above, combining DP with SA is a comparatively effective method for protecting privacy. Combining DP with BR in FL offers a potential solution. However, the previous approaches for achieving DP and BR are incompatible, as demonstrated by Guerraoui et al. . Specifically, the theoretical conflict arises from the interference between DP noise and BR detection mechanisms. For example, directly superimposing DP noise blurs gradient directions (e.g., invalidating cosine similarity-based detection), which could cause BR methods to misclassify benign updates as malicious (false positives) or fail to detect carefully crafted Byzantine gradients (false negatives). They further found that directly combining DP and BR techniques causes the guarantees of the distributed SGD algorithm to depend unfavorably on model size. This makes the training of large-scale models practically infeasible. Our work addresses the challenge: how to combine the properties of DP and BR while maintaining the global model quality as much as possible?\nTo address the incompatibility between privacy and robustness in FL, we introduce secondary servers between users and the central aggregator. Unlike prior methods that burden users or the aggregator with both tasks, our design decouples responsibilities: secondary servers first validate model updates via Byzantine detection using MPC, then aggregate the validated updates with verifiable DP noise. This sequential approach ensures robust aggregation is performed on clean data before noise injection, which is crucial because BR mechanisms depend on precise gradient statistics. DP noise, if injected prematurely, could distort these statistics. Additionally, noise levels can be minimized once malicious updates are filtered out.\nBuilding on this architecture, our scheme separates robustness evaluation from privacy preservation by employing secret sharing-based secure aggregation to protect local model privacy while applying DP mechanisms only to the global model. This separation reduces overall noise and enhances accuracy, overcoming limitations of methods that directly combine DP and BR.\nWe also propose a Noise-Immune Shamir Secret Sharing protocol that enables verifiable noise addition while ensuring correct secret reconstruction-a feat traditional Shamir schemes cannot achieve under noisy conditions. By isolating and integrating DP and BR components, our framework optimally balances privacy, robustness, and model utility. Experimental results show that our approach effectively defends against Byzantine and privacy inference attacks while improving convergence and overall performance, making it a promising solution for secure and scalable federated learning.\nWe highlight our contributions as follows:\nr A novel FL architecture with intermediary secondary servers that decouple Byzantine resilience from differential privacy. This is the first framework enabling both verifiable DP and BR without mutual interference."
    },
    {
      "title": "RELATED WORKS",
      "content": ""
    },
    {
      "title": "Byzantine Resilient FL",
      "content": "To defend against poisoning attacks, which encompass both data poisoning and model poisoning, several BR schemes have been proposed , , , , , . Yin et al. proposed an assumption based on the statistical differentiation between honest and Byzantine models, which led to the design of a robust federated aggregation algorithm known as Krum . Dong et al. proposed the Π P2BROFL protocol, which combined the Shuffle protocol with a malicious Top-K strategy. This enhanced the Multi-Krum protocol's resistance to poisoning attacks, limiting the model accuracy drop to a mere 1.05% even when up to 50% of users were malicious . Tang et al. introduced PILE, which utilized zero-knowledge proofs (ZKPs) to verify dummy model gradients without decryption. However, PILE is designed to defend against model poisoning attacks but not data poisoning, as it does not validate local datasets. These works comprehensively analyzed poisoning attacks from diverse perspectives and, recognizing their unique characteristics, devised a variety of potent defenses accordingly. In this work, we adopt norm-based and cosine similarity-based BR methods for easy implementation on MPC circuits."
    },
    {
      "title": "Privacy Preserving FL",
      "content": "Currently, numerous privacy-preserving mechanisms are employed to counter privacy leakage in FL, which can be broadly categorized into cryptographic primitives , , and DP techniques , , . Bonawitz et al. devised a practical SA algorithm, SecAgg, based on threshold Shamir secret sharing. This algorithm efficiently safeguards the privacy of local models against honest-but-curious servers. Given that SA cannot ensure the global model remains secret throughout the training process in FL, and considering the immaturity of homomorphic encryption (HE) for this purpose, DP remains the widely recognized method for preserving privacy in FL.\nTo balance the global model accuracy and privacy utility in DP-based FL, some studies aim to integrate the properties of SA and DP , . SA preserves the privacy of local models and ensures lossless aggregation, while DP preserves the privacy of the global model but may decrease accuracy. Kairouz et al. analyzed the attributes of the discrete Gaussian mechanism for differential privacy, highlighting that the sum of multiple discrete Gaussian noises approximates a discrete Gaussian distribution. Leveraging this characteristic, they have integrated SA with DP, outlining an interpretable and manageable federated secure aggregation scheme that achieves 1 2 ε 2 -concentrated DP for the global model, demonstrating a mean squared error bound of O( c 2 d ε 2 ). Building on this work, Chen et al. employed the Johnson-Lindenstrauss lemma to implement sparse random projections and counting sketches, which reduced vector dimensions and information redundancy to enhance data privacy protection. However, these works predominantly assumed semihonest FL users, failing to account for the possibility that, in realworld scenarios, malicious adversaries might tamper with the global model parameters by introducing fraudulent noise. This vulnerability potentially heightens the risk of privacy breaches for honest users. To address this vulnerability, we introduce verifiable differential privacy as a safeguard in our work."
    },
    {
      "title": "Byzantine Resilient and Privacy Preserving FL",
      "content": "In pursuit of combining BR with privacy preservation methods, many works have achieved comprehensive BR technologies employing MPC circuits , , , . Corrigan et al. introduced Prio , which employed Shamir secret sharing to implement a generic validation circuit named SNIP. Building upon Prio, Roy et al. proposed EIFFeL , enhancing it with homomorphic commitment technology as a defense against malicious adversaries. However, they did not consider the risk of privacy leakage from the global model. Zhou et al. formulated a dynamic differential privacy protocol aiming to strike a balance between privacy preservation and model utility, while also integrating Byzantine-resilient methods to repel malicious poisoning attacks. Nonetheless, it might not have fully optimized model accuracy, given that the dynamic DP protocol prioritized establishing a minimum level of DP noise, potentially sacrificing model performance. Moreover, their work did not account for the possibility that the adversary could have corrupted FL training by falsifying the size of the noise. When evaluating FL in terms of privacy preservation, model utility, and defense against malicious attacks, it becomes clear that current methods often excel in one or two of these areas but have not yet achieved an ideal balance across all three."
    },
    {
      "title": "PRELIMINARIES",
      "content": ""
    },
    {
      "title": "Notations",
      "content": "In this section, we provide a summary of the key symbols used throughout this paper. TABLE I lists the important notations, along with their definitions, to facilitate understanding of the formulas and concepts presented in the subsequent sections."
    },
    {
      "title": "Federated Learning",
      "content": "Federated Learning (FL) is distinguished into two primary categories: Cross-silo FL and Cross-device FL . In this work, we particularly focus on the Cross-device FL. The workflow of FL generally adheres to the following steps: \nw = u∈U q u • w u , (1\n)\nwhere w u is the vector of the local model parameters of the user u, and q u is their weight, determined by the size of the user's dataset. And q u is published to all the participants. 5) Round until Convergence: Steps 2 through 4 are repeated over multiple communication rounds until the global model converges to a satisfactory performance level or until predefined stopping criteria are met."
    },
    {
      "title": "Pedersen's Verifiable Secret Sharing",
      "content": "Pedersen's Verifiable Secret Sharing (VSS) enhances Shamir's threshold secret sharing by adding verification steps that ensure the correctness and authenticity of the distributed secret. Informally, Π pvss is the Pedersen's VSS protocol involving five algorithms.\nr Ped.Setup(n, λ) → pp = (p, q, g, h, t): Given the num- ber of participants n and the security parameter λ, this algorithm products the public parameters pp, which its general process is as follows: -Generates public parameters including two large primes p and q (with q dividing p -1, forming the finite field Z p ). -Chooses two generators g and h for the subgroup G q . -Sets the threshold t which defines the minimum number of shares required to reconstruct the secret.\nr Ped.Com(v, r) → c: This algorithm creates a commit- ment c to a message v ∈ Z q and randomness r ∈ Z q . The commitment scheme used is based on the discrete logarithm problem, ensuring that the commitment is binding and hiding. It also verifies the correctness of the reconstructed secret through the commitments cs and outputs a boolean value b ∈ {0, 1} to confirm the integrity of the secret. Pedersen's VSS has an additive homomorphic property. This property allows it to integrate seamlessly with tools like Beaver triples for MPC. Pedersen's VSS also provides robust protection against passive adversaries and defends against active adversaries who try to steal or disrupt the secret. This ensures both the confidentiality and integrity of the secret. Typically, we set the threshold t as n/3 ≤ t ≤ n/2 to maintain system robustness even when some participants are dishonest."
    },
    {
      "title": "Verifiable Differential Privacy",
      "content": "Verifiable differential privacy ensures the randomness and unbiasedness of noise in untrusted environments. Approaches like VerDP , which uses the Fuzz query language, and DPrio , which integrates differential privacy into share distribution based on Prio, address this need. Biswas et al. enhanced the approach to prevent active adversaries using the Σ-Protocol, and proved the impossibility of perfect verification with unlimited computational power. We adopt and revise the approach of Biswas et al. in this work. The protocol requires the prover s and verifier A to negotiate and sum verifiable random coins to achieve verifiable Binomial DP through the following steps.\nr Setup: The provers and verifier generate public parameters.\nThe client sending an additive share of its secret value to the prover and the commitment of the share to the verifier.\nr Generate private coins: The prover s samples private bits b s,j , j ∈ [n b ] and sends their commitment c s,j = Ped.Com(b s,j , r s,j ) to the verifier.\nr Check the update from the prover: The verifier checks the commitments c s,j from the prover using an oracle O OR .\nr Generate public coins: If the checks pass, both parties obtain public random and unbiased bits v s,j using an oracle O Morra .\nr Obtain the verifiable noise: The prover XORs the private and public coins to obtain the verifiable noise v s :\nv s = n b j=1 (b s,j ⊕ v s,j ) , r s = n b j=1 (-1) v s,j r s,j . (2)\nAnd the verifier calculates the commitment c s of the noise:\nc s = n b j=1 Ped.Com(1, 0) * c -1 s,j , If v s,j = 1; c s,j , Otherwise.(3)\nIn the above protocol, the verifier ensures the integrity of the noise using commitment c s , while the prover keeps the privacy of the noise with random number r s . This protocol involves two oracles. First, the oracle O OR confirms that c s,j indeed commits to the secret b s,j , which must be either 0 or 1. However, it suffers from a security vulnerability due to its use of two incompatible Σ-protocols in verification, potentially allowing adversaries to steal the secret b s,j by analyzing verification messages (details can be found in ). Second, the oracle O Morra is responsible for generating unbiased public coins but incurs a high communication cost.\nTo address these concerns, we improve the above two oracles as follows:\nr Verification employs the secure protocol Π OR rather than the vulnerable O OR , which is based on the property that\nx(x -1) = 0 ⇐⇒ x ∈ {0, 1}.\nr Under the semi-honest assumption about the aggregator, public coins can be directly generated by the aggregator and distributed to secondary servers, thereby reducing communication costs."
    },
    {
      "title": "SYSTEM MODEL",
      "content": "Fig. 1 illustrates our DPSFL scheme with three entities: Users, Secondary Servers, and an Aggregator. Unlike traditional FL systems with only users and a central aggregator, the introduction of secondary servers is essential for ensuring both privacy and Byzantine resilience. We conduct a theoretical analysis of the necessity of secondary servers in Section VI-A.\nAuthorized licensed use limited to: Kennesaw State University. Downloaded on October 13,2025 at 18:38:37 UTC from IEEE Xplore. Restrictions apply. adding the negotiated DP noise, and forwarding the resulting noisy aggregated share to the aggregator. Aggregator (A): Its tasks are as follows: 1) Verification: Checks the correctness of both the verifiable DP noise and the robustness evaluation carried out by the secondary servers. 2) Global Model Reconstruction: Verifies the received noisy shares (using commitments for both the shares and the noise) and reconstruct the noisy aggregate, which is used as the updated global model. We consider a scenario where a malicious adversary may control a subset of users and a limited number of secondary servers, while the aggregator is assumed to be honest-butcurious. Specifically, we assume that:\nMalicious Users: Users may attempt to poison the model, manipulate updates, or collude with Secondary Servers. 2) Compromised Secondary Servers: A fraction of secondary servers (fewer than t in quantity) may attempt to distort the robustness evaluation or reconstruct user model updates. 3) Curious Aggregator: The aggregator is honest-but-curious and follows the protocol but may attempt to infer private information. As the core coordinating party, the aggregator can reduce the risk of malicious behavior by leveraging trusted hardware (such as Intel SGX ) or regulatory constraints. These assumptions reflect realistic scenarios in which critical infrastructure (e.g., edge hubs or cloud secondary servers) is better protected than end-user devices."
    },
    {
      "title": "PROPOSED SCHEME",
      "content": "In this section, we formally introduce our scheme. Firstly, we introduce the design overview of our scheme in Section V-A. Secondly, we propose the protocol Π ln for the noise-immune Shamir secret sharing in Section V-B. Finally, we then propose the details of our DPSFL scheme in Section V-C and the correctness analyzing in Section V-D."
    },
    {
      "title": "Design Overview",
      "content": "Current FL architectures struggle to harmonize DP and BR due to inherent statistical conflicts and centralized bottlenecks. While prior works , attempt to combine DP noise injection with Byzantine-robust aggregation rules, they face two critical limitations: (1) DP noise distorts gradient statistics essential for BR detection mechanisms (e.g., invalidating cosine similarity metrics), and (2) centralized aggregators become single points of failure that undermine both privacy and robustness guarantees. Our DPSFL framework introduces three interconnected innovations to overcome these challenges.\nSequential Validation-Noise Pipeline: We decouple BR validation from DP noise injection via intermediary secondary servers. Specifically, secondary servers first perform MPC-based Byzantine detection on raw updates of users (Phase 3) before applying verifiable DP noise to aggregated shares (Phase 4). This sequential approach preserves gradient statistics for accurate BR detection while confining DP perturbations to the final aggregation step.\nNoise-Immune Shamir Secret Sharing: Traditional Shamir schemes fail under noisy conditions due to Lagrange interpolation errors (Section VII-A). Our protocol embeds binomial noise directly into secret shares using pairwise PRF masking, enabling correct aggregation while satisfying (ε, δ)-DP. This \nReceives V s s ∈ S t . w u ← s∈S t ([w u ] s Δ s S t + V s ).\nallows verifiable noise addition without exposing raw gradients to the aggregator.\nDistributed Workload Allocation: We offload computational heavy operations (Pedersen commitment verification, SNIP validation circuits) to secondary servers through parallelizable MPC protocols while users only perform lightweight operations.\nThe protocol progresses through four phases: Phase 1: Users split local models into verifiable Shamir shares using Pedersen VSS, ensuring privacy against compromised secondary servers (Section V-C).\nPhase 2: Secondary servers negotiate binomial noise with the aggregator.\nPhase 3: Collaborative MPC validation filters malicious updates using a BR method (such a norm-based or cosine similarity checks) before any noise injection, addressing the statistical conflict of DP and BR.\nPhase 4: The aggregator reconstructs the global model from verified noisy shares using Lagrange interpolation. To prevent excessive noise amplification, DPSFL leverages controlled noise injection and adaptive thresholding techniques.\nDPSFL balances model accuracy, privacy, and robustness by integrating noise into secret sharing and leveraging hierarchical validation. This design supports scalability and adversarial resilience in large-scale edge deployments."
    },
    {
      "title": "Noise-Immune Shamir Secret Sharing",
      "content": "In Shamir's secret sharing scheme, Lagrange interpolation is conventionally employed for secret reconstruction. Consider a perturbed share defined as [w] s = [w] s + v s for each secondary server s ∈ S, where [w] s denotes the original share and v s represents additive perturbation introduced by secondary server s. Through Lagrange interpolation, the reconstructed secret exhibits the following error propagation characteristic:\nw org = s∈S t [w] s Δ s S t = w + s∈S t v s Δ s S t .\nWhere the Lagrange coefficient\nΔ s S t = s ∈S t \\{s} x s x s -x s demonstrates factorial scaling behavior. Specifically, Δ s S t = t t-1 (t-1)! ∝ e t √\nt via Stirling approximation, implying that the aggregated perturbation grows super-exponentially with threshold parameter t. Empirical validation of this error amplification phenomenon is provided in Section VII-A.\nTo address this t-dependent error amplification, we propose an optimized reconstruction operator:\nw opt = s∈S t [w] s Δ s S t + s∈S t v s -v s Δ s S t = w + s∈S t v s .\n(4) This construction effectively decouples the final reconstruction error from the threshold parameter t. The formal specification of our Noise-Immune Shamir Secret Sharing protocol Π ln appears in Algorithm 1. We then prove the correctness in Section V-D."
    },
    {
      "title": "Design of DPSFL",
      "content": "The full protocol is presented in Fig. 2. Our scheme involves a setup phase and the following four phases.\nSetup Phase: Given a security parameter λ and the set of secondary servers S, the aggregator runs Ped.Setup(|S|, λ) → (p, q, g, h, t), and generates two empty sets S M , U M to store the malicious party. To keep log g h private, and to generate the key for the parties, we split this step as follows:\nr The aggregator A and the secondary servers s ∈ S generate their own secret number a, a s ∈ Z q for s ∈ S; r The aggregator A and the secondary servers s ∈ S calcu- late h = g a s∈S a s together. r The aggregator A checks the integrity of the private coins using the protocol Π OR . r If checks pass, the aggregator A sends the public coins v s,j to the secondary server s. Otherwise, the aggregator regards the secondary server s as the malicious party and s → S M .\nr The secondary server s calculates the verifiable noise (v s , r s ) using the (2), where v s is the raw noise. The aggregator A calculates the commitment of the noise c s =: Ped.Com(v s , r s ) using the (3)."
    },
    {
      "title": "hase 3. Evaluating the Integrity and Byzantine Robustness of Local",
      "content": "In this phase, the secondary server s ∈ S first checks the commitment and the share from users u ∈ U using Ped.Verify(•) → b.\n• If there is more than |S|t secondary servers to obtain the result b = 0, we then regard the user u as a malicious adversary and u → U M .\nNext, to evaluate the robustness of the local models, we first construct an interactive evaluation of the function Valid(•) using the MPC methods, as in SNIP . Because of the homomorphism of Pedersen's VSS, we can further design a verifiable Valid(•) by synchronously calculating the commitments of the intermediate and final values of the function Valid(•) and verify the integrity of the evaluation.\n• If a secondary server s identifies another secondary server s as the abnormal party during the evaluation, the aggregator A then verifies the integrity of the evaluation. If the verification fails, we classify s as the malicious party; otherwise, we classify s as the malicious party.\nBased on Valid(w u ), we decide whether the local model w u participates in the current round of aggregation and the user u involves the set U H .\nPhase 4. Aggregation and Verification: In this phase, the secondary servers s ∈ S first aggregate the share and add the noise as follows:\n[w] s = u∈U H (q u • [w u ] s ) + v s ; [r] s = u∈U H (q u • [r u ] s ) + r s .\n(5) Where q u is the weight of user u determined by the size of the user's dataset and u∈U H q u = 1.\nSecondly, the secondary servers s sends ([w] s , [r] s ) to the aggregator A, A checks these share of noisy parameters of the global model using the commitment as follows:\nPed.Com ([w] s , [r] s ) ? = u∈U H t-1 k=1 c x k s u,k × c u q u × c s . (6)\nNext, the aggregator A chooses t secondary servers s ∈ S t , where s ∈ S t is selected from the set of secondary servers that can be executed normally in the previous steps. The aggregator A and the set of secondary servers S t interactively reconstruct the noisy parameters of the global model (w , r ) using the protocol Π ln . (Since the protocol Π ln requires two passes of execution, we label the intermediate variables V s and R s to distinguish between the two passes. Note that V s represents the intermediate variable at the first execution of the protocol, while R s is the intermediate variable at the second execution.)\nFinally, the aggregator A then checks the correctness of the noisy parameters of the global model (w , r ) using the commitment as follows:\nPed.Com (w , r ) ? = u∈U H c q u u × s∈S t c s . (7\n)"
    },
    {
      "title": "Correctness of DPSFL 1) Aggregation Correctness With Noise",
      "content": "The correctness of the aggregation directly follows from the correctness of the protocol Π ln . We prove the correctness of the protocol Π ln as follows:\nw = s∈S t [w] s Δ s S t + s∈S t v s -v s Δ s S t = s∈S t ([w] s + v s ) Δ s S t + v s -v s Δ s S t = w + s∈S t v s ,(8)\nwhere\n[w] s = u∈U H (q u • [w u ] s )\nis the share of the raw global model, and w = u∈U H q u • w u is the raw global model. Furthermore, the noise s∈S t v s is the sum of verifiable binomial noises, each of which follows a Binomial(n b , 1 2 ) distribution. Consequently, s∈S t v s follows a Binomial(n b • t, 1 2 ) distribution.\nTherefore, w is the noisy global model that satisfies our goals and is correct.\nSimilarly, we have:\nr = u∈U H q u • r u + s∈S t r s . (9\n)\nVerification Correctness: We now discuss the correctness of the ( 6) and (7).\nFor the (6), because t-1 k=1 c\nx k s u,k × c u = Ped.Com([w u ] s , [r u ] s ) according to the algorithm Ped.Verify(•), and c s = Ped.Com(v s , r s ) according to the protocol of verifiable DP, we can get that:\nPed.Com([w] s , [r] s ) = Ped.Com u∈U H q u • [w u ] s + v s , u∈U H q u • [r u ] s + r s = u∈U H Ped.Com ([w u ] s , [r u ] s ) q u ×c s = u∈U H t-1 k=1 c x k s u,k × c u q u × c s .\nBased on ( 8) and ( 9), the left hand side of ( 7) is that:\nPed.Com (w , r ) = Ped.Com w + s∈S t v s , u∈U H q u • •r u + s∈S t r s = Ped.Com u∈U H q u • w u , u∈U H q u • r u × s∈S t c s = u∈U H c q u u × s∈S t c s .\nTherefore, the correctness of ( 6) and ( 7) is guaranteed.\nAuthorized licensed use limited to: Kennesaw State University. Downloaded on October 13,2025 at 18:38:37 UTC from IEEE Xplore. Restrictions apply."
    },
    {
      "title": "THEORETICAL ANALYSIS",
      "content": ""
    },
    {
      "title": "Theoretical Analysis of the Necessity of Secondary Servers",
      "content": "In conventional FL architectures (user-aggregator systems), three methods exist to achieve both DP and BR. We analyze their fundamental limitations to demonstrate the necessity of introducing secondary servers.\nCase 1. Local DP with BR: In this case, users inject DP noise into their local updates before transmitting them to the aggregator, which subsequently applies BR mechanisms. While theoretically appealing, this approach suffers from a critical statistical incompatibility as demonstrated in recent studies , . Specifically, the introduction of DP noise significantly increases the variance of updates (such as gradients), severely disrupting the statistical properties used to identify malicious (Byzantine) updates.\nCase 2. BR with Global DP: This approach employs SA to protect individual updates during transmission, followed by BR filtering at the aggregator and global DP on the final model. While SA prevents direct access to individual gradients, critical privacy risks persist:\nFirst, even with SA protecting individual updates, the exact global update (e.g. gradient) becomes accessible to the aggregator after decryption. Through iterative observations of unprotected global updates across rounds, curious aggregator can reconstruct individual contributions through inference attacks, such as membership inference or privacy reconstruct attack . This risk escalates in long-running training processes, as cumulative information leakage violates the compositional privacy principle-a flaw unaddressed by SA alone.\nSecond, the architecture further introduces contradictory trust assumptions. The aggregator must be trusted to faithfully execute BR faithfully using raw updates (to ensure robustness) while simultaneously being distrusted from exploiting those same updates to infer private information (to ensure privacy). This paradox cannot be efficiently resolved within a conventional FL architecture, as highlighted in . SA alone cannot mitigate this conflict, as it only secures the aggregation process and does not protect the privacy of the decrypted global states. Thus, global DP must directly perturb the aggregated model to break statistical correlations, but doing so after BR operations fails to protect intermediate results.\nCase 3. Fully Decentralized MPC Implementation: A theoretically viable alternative employs secure MPC among users to jointly implement BR and DP without centralized aggregators. However, practical implementation faces insurmountable scalability barriers. For n participants, MPC protocols like SPDZ require O(n 2 ) communication complexity per training round due to pairwise consistency verifications-a prohibitive overhead for large-scale FL systems with n > 10 3 users. Moreover, resource-constrained edge devices cannot sustain the cryptographic computations required for high-dimensional models (e.g., neural networks with d > 10 6 parameters), as shown by latency measurements in . These limitations reduce theoretical guarantees to impractical ideals, particularly for time-sensitive applications.\nIn summary, the three aforementioned approaches for achieving both DP and BR in conventional FL architectures are all impractical. The necessity of secondary servers thus emerges not as an optional optimization but as a foundational requirement for achieving practical, privacy-preserving, and Byzantine-resilient FL systems. No existing conventional FL architecture or fully decentralized protocol can meet these requirements simultaneously without introducing significant trade-offs in security, efficiency, or statistical validity."
    },
    {
      "title": "Cryptographic Assumptions and Definitions",
      "content": "Definition 1 (Discrete Logarithm Assumption (DLA) , ): Let G be a finite cyclic group of order n with generator α, and let β ∈ G. The discrete logarithm of β to the base α, denoted log α β, is the unique integer x such that β = α x mod n. The discrete logarithm problem is considered hard if for any probabilistic polynomial-time adversary M, there exists a negligible function negl(n) such that\nPr [M(α, β) = x] ≤ negl(n).\nDefinition 2 ((ε, δ)-Differential Privacy ): A randomized mechanism M : X → Y satisfies (ε, δ)-differential privacy if for any two adjacent inputs x, x ∈ X differing in at most one element and any subset S ⊆ Y ,\nPr [M (x) ∈ S] ≤ e ε Pr [M (x ) ∈ S] + δ."
    },
    {
      "title": "Differential Privacy Analysis",
      "content": "Lemma 1 (Binomial Mechanism , ): Let X = (x 1 , . . . , x n ) ∈ Z n q and define the counting query\nQ(X) = n i=1 x i . Fix n b > 30, 0 < δ ≤ O(1/n b )\n, and let Z ∼ Binomial(n b , 1 2 ). Then Q(X) + Z satisfies (ε, δ)-differential privacy with ε = 10 1 n b ln 2 δ . A detailed derivation follows the proof structure in . Theorem 1 (Global Model Privacy): In DPSFL, the global model update is computed as\nw = w + s∈S t v s ,\nwhere w is the aggregated model update from honest users and v s is the noise injected by secondary server s. Under the Binomial Mechanism, the global model satisfies (ε, δ)-differential privacy with\nε = 10 1 t • n b ln 2 δ ,\nwhere t is the threshold for secret reconstruction, and n b is the per-server noise parameter. Proof: Since each secondary server injects noise v s ∼ Binomial(n b , 1 2 ), the total noise added to the global model is\ns∈S t v s ∼ Binomial(t • n b , 12\n). Applying Lemma 1 with total noise parameter t • n b , the privacy guarantee follows directly."
    },
    {
      "title": "Impact of Secondary Servers on Privacy Corollary 1 (Impact of Secondary Servers on Privacy)",
      "content": "Let the global model satisfy (ε, δ)-DP with noise generated collaboratively by |S| secondary servers under threshold t. For fixed ε, if t = Θ(|S|) (e.g., t = |S| 2 + 1), then the per-server noise parameter n b scales as n b = O( 1|S| ). Thus, increasing |S| reduces individual noise requirements while preserving privacy.\nProof: From Theorem 1, the global model guarantees (ε, δ)-DP with: ε = 10\n1 t•n b ln 2 δ . Rearranging for t • n b : t • n b = 100 ε 2 ln 2 δ C(ε, δ),\nwhere C(ε, δ) is a constant for fixed ε, δ.\nUnder the threshold scheme t = Θ(|S|), let t = k|S| for constant 1/3 ≤ k ≤ 1/2 (e.g., k = 1/2 for t = |S|/2 + 1). Substituting t = k|S| into the equation:\nk|S| • n b = C(ε, δ) ⇒ n b = C(ε, δ) k|S| .\nThus,\nn b = O( 1 |S|\n). This implies: 1. For fixed ε, increasing |S| linearly reduces the per secondary server noise parameter n b . 2. The total noise budget t • n b = C(ε, δ) remains constant, preserving the global (ε, δ)-DP guarantee. 3. Distributed noise generation across |S| secondary servers avoids single-point bottlenecks while maintaining privacy."
    },
    {
      "title": "Information-Theoretic Security",
      "content": "Definition 3 (Information-Theoretic Security (IT-Security) ): A cryptosystem is information-theoretically secure if its security does not rely on computational assumptions. That is, even an adversary with unlimited computational power cannot break the system, except by directly accessing the secret key.\nTheorem 2 (Security Under Collusion): The colluding malicious users (u M ∈ U M ) and a few secondary servers (s M ∈ S M , |S M | < t) are unknown for any information of another user under IT-Security and the DLA.\nProof: Firstly, the aggregator A is not under adversarial control and does not collude with malicious parties. Thus, we do not consider the security threat of the aggregator in this proof.\nAs we known, such as Shamir secret sharing are informationtheoretically secure in that having less than the requisite number of shares of the secret provides no information about the secret .\nThe adversary can perform two types of attacks: passive (eavesdropping and analytical inference) and active (protocol manipulation)."
    },
    {
      "title": "assive listening and analytical attack",
      "content": "In this way, the adversary passively collects information and attempts to analyze the privacy of honest users. It can obtain the following information about user u during training: Based on information-theoretic security, the adversary cannot gain any information about the user u from the share ([w u ] s , [r u ] s ). Furthermore, the adversary cannot gain any information from the commitment: (cs u ), unless it can solve the discrete logarithm problem. During the process of calculating Valid(•), the secondary servers interactively perform the multiplication by using the multiplication triple. However, the information in the interaction is random and does not contain any privacy. Therefore, the passive adversary does not gain any privacy about the honest user u.\nActive attack against the protocol: In this way, the adversary can attack the protocol to obtain information or even break it. Consider a malicious adversary M who controls some users u M ∈ U M and a few secondary servers s M ∈ S M , where |S M | < t. The adversary M can perform the following attacks:\nDoS/DDoS Attack: The adversary M can initiate Denial-of-Service (DoS) or Distributed Denial-of-Service (DDoS) attacks leveraging controlled users and secondary servers. These attacks aim to make the system inaccessible by overwhelming it with traffic or requests that exceed its processing capacity. 2) Data tampering: The adversary M can alter the data submitted by the users and the secondary servers to distort the results or mislead other parties. This attack can undermine the trustworthiness and availability of the system. The adversary can tamper following data:\nr ([w u M ] s , [r u M ] s , cs u M ):\nThe data from malicious users\nu M . r {P s M ,j , c s M ,j |j ∈ [n b ]}:\nThe data from malicious sec- ondary servers s M during the phase 2. Verifiable Noise Negotiation.\nr The interaction data with other secondary servers in phase 3. Evaluating the Integrity and Byzantine Robustness of Local Models.\nr ([w] s M , [r] s M , V s M , R s M ):\nThe data from malicious secondary servers s M during aggregation. For the DoS / DDoS attacks, the DPSFL has a powerful detection and filtering mechanism. If some users drop out, the honest secondary servers report this occurrence to the aggregator and remove the updates related to the dropped users. If the updates from malicious secondary servers do not align with those from most other secondary servers or if they drop out, the aggregator will broadcast the IDs of these secondary servers and exclude them from the training process.\nFor data tempering, our scheme incorporates a verification mechanism based on Pedersen's Commitment into all enumerated updates to verify their correctness. Therefore, an adversary cannot temper with any updates unless it can solve the discrete logarithm problem.\nIn summary, Theorem 3. is proved to be correct."
    },
    {
      "title": "Trade-Offs Between Differential Privacy and Byzantine Resilience",
      "content": "The DPSFL framework theoretically reconciles DP and BR through three interconnected mechanisms.\nFirst, its hierarchical architecture isolates BR validation from DP noise injection: Byzantine detection is performed on raw gradients using Secure MPC to preserve detection accuracy, while binomial noise is applied solely to the aggregated model. This prevents DP perturbations from distorting BR statistical analysis (Theorem 2).\nSecond, cryptographic protocols (e.g., Π OR proofs, Shamir sharing) ensure adversarial collusion cannot bypass BR checks or reconstruct raw updates (local and global), securing both phases against adaptive threats (Theorem 2).\nAdjustable parameter tuning further harmonizes these guarantees-tightening BR thresholds under attacks while proportionally adjusting n b to maintain DP-via verifiable MPC protocols. By confining BR to raw data and DP to aggregated outputs, DPSFL eliminates their mutual interference, achieving provable equilibrium between privacy, robustness, and utility."
    },
    {
      "title": "EXPERIMENTS",
      "content": "In this section, we empirically examine the accuracy of the global model when different noise sizes are added to DPSFL. Next, we evaluate its integrity using common BR methods and a series of attacks. Additionally, we compare the accuracy of the global model with different proportion of adversaries in DPSFL to the work of Zhou et al. .\nExperiments are conducted on a single NVIDIA A5000-24 G GPU with AMD EPYC 7551P processors. Our scheme is implemented in Python using the PyTorch library, with cryptographic operations handled by the PyCrypto library ."
    },
    {
      "title": "Evaluation on Noise of the Lagrange's Interpolation",
      "content": "To assess the noise amplification effect in the secret recovery phase of noise-immune Shamir secret sharing, we design a comprehensive simulation experiment as outlined here:\nThe experimental setup involves a single secret holder and n participants (dealers) within a secret sharing framework. The secret holder possesses a confidential value x, which is initially disseminated among all dealers in accordance with an (n, t)threshold Shamir secret sharing scheme. Subsequently, each dealer introduces noise to their respective share, where the noise is modeled by a Binomial distribution Binomial(n b , 1 2 ), symbolizing a controlled disturbance. These augmented shares are then transmitted to a central server. The server, employing Lagrange interpolation, attempts to reconstruct the original secret, yielding an estimate x . A pivotal aspect of our analysis entails comparing x with x to quantify the magnitude of noise amplification, thereby elucidating how the introduced noise propagates through the reconstruction process.\nIn exploring the behavior of noise amplification under varying noise profiles and thresholds t, we establish a noise magnitude spectrum defined by ε = {1, 2, 3, 4, 5} and δ = 10 -32 . The values of n = t both range from 3 to 50. The result of this experiment is as Fig. 3. We found that the noise size grows exponentially with the number of threshold t for ε ranging from 1 to 5. This means that we cannot recover the secret value when the secret share is noisy."
    },
    {
      "title": "Evaluation on Accuracy",
      "content": "In the following experiment, we evaluate DPSFL on three datasets (MNIST, FMNIST and CIFAR-10), and divide the datasets by using Dirichlet distribution (α = 0.5) to obtain non-IID datasets for FL training. The specific descriptions of the datasets and corresponding models are as follows:\nDatasets:\nr MNIST is a handwritten digit recognition dataset con- taining 60,000 training images and 10,000 testing images of size 28x28 pixels, each labeled with one of ten digits from 0 to 9.\nr FMNIST (Fashion-MNIST) r We re-designed a small VGG net with 14 layers and 5, 621K parameters for CIFAR-10. To investigate the impact of noise intensity on federated learning performance, we configure a system with |U | = 50 users and |S| = 5 secondary servers, employing a user dropout rate of 0.01. Fig. 4(a)-(c) presents the per-round accuracy trajectories under differential privacy constraints (ε ∈ {1.0, 0.5, 0.1, 0.05}, δ = 10 -32 ) and a noiseless baseline across MNIST, Fashion MNIST, and CIFAR-10 datasets.\nThe experimental results (Fig. 4) demonstrate the impact of DP noise levels (ε = 1.0, 0.5, 0.1, 0.05) on model accuracy and convergence across three benchmark datasets: MNIST, FM-NIST, and CIFAR10. The analysis reveals significant variations in noise sensitivity depending on task complexity and training phases.\nNoise Robustness Across Architectures: For MNIST (MLP model, Fig. 4(a)), the accuracy remains at 98.12% under ε = 1.0 (baseline: 98.24%) and 97.59% even at ε = 0.05, attributed to the global feature modeling capability of fully connected layers on low-dimensional grayscale data. In contrast, Fashion MNIST (LeNet-5 model, Fig. 4(b)) exhibits significant accuracy degradation to 88% at ε = 0.05 (baseline: 91.5%), primarily due to the sensitivity of its shallow convolutional structure (e.g., 5 × 5 kernels in the first layer) to local texture perturbations. CIFAR-10 (VGG-14 model, Fig. 4(c)) shows unexpected robustness, with only a 2% drop at ε = 0.1 (73% vs 75% baseline), suggesting that hierarchical nonlinear transformations in deep convolutional networks can partially buffer noise propagation, albeit with notable late-training instability.\nConvergence Dynamics and Stability: Under identical noise levels, convergence efficiency decreases with task complexity. MNIST/MLP (Fig. 4(d)) stabilizes within 50 rounds (loss 0.1 ± 0.02), while CIFAR-10/VGG-14 (Fig. 4(f)) requires 65 rounds at ε = 0.1 (18% longer than baseline), accompanied by significant late-stage accuracy fluctuations (standard deviation ±1.8% vs baseline ±0.6%). Further analysis reveals that DP noise amplifies gradient norms in the classification layers of deep networks, destabilizing parameter updates.\nPractical Recommendations: Based on these findings, we propose: (1) For shallow convolutional models (e.g., LeNet-5), set ε ≥ 0.1 to prevent feature extraction degradation; (2) Deep networks (e.g., VGG-14) benefit from extended training (20% additional rounds) and progressive noise decay (e.g., linearly reducing to 30% of initial noise in the final 10% of training), which experimentally reduces CIFAR-10 fluctuations to ±1.1%;\n(3) Under high-privacy constraints (ε = 0.05), task viability varies: MNIST/MLP (Fig. 4(a)) maintains > 97.5% accuracy, whereas CIFAR-10/VGG-14 (Fig. 4(c)) is limited to 69.2% absolute accuracy (5.8% drop from baseline)."
    },
    {
      "title": "Evaluation on Integrity",
      "content": "To analyze the integrity of our scheme in the setting of malicious parties, we compare DPSFL with EIFFeL . According to the above conclusion, we set the noise size ε = 0.1.\nAdditionally, we further select various effective BR methods, such as norm ball , cosine similarity validation , and FLAME . To implement these defense strategies within MPC circuits, we initially note that the pivotal operations in these techniques all involve computing the dot product of two vectors, which may be held as secret shares among the parties. We recognize that the dot product's result does not compromise the vectors' privacy without prior knowledge of their relationships. Our goal is to create a function that computes the dot product of two secret-shared vectors and then reveals the dot product's result for further processing.\nThe comparative evaluation of DPSFL and EIFFeL under 30% malicious users reveals nuanced performance trade-offs shaped by their respective defense mechanisms. As shown in Fig. 5(a)-(e), the two frameworks exhibit nearly overlapping accuracy trajectories across additive noise, sign-flipping, and gradient manipulation attacks. This parity suggests that the shared BR methods-norm clipping, cosine similarity validation, and FLAME-dominate robustness outcomes in this threat regime. The marginal performance gap (< 0.5% across datasets) further indicates that DPSFL's calibrated DP noise neither degrades nor enhances robustness significantly when applied alongside these BR techniques, effectively preserving utility while enforcing privacy.\nDivergence emerges in backdoor attack resilience (Fig. 5(g)). While EIFFeL achieves a 1% higher main-task accuracy on CIFAR-10, DPSFL reduces backdoor success rates by 5%-a Authorized licensed use limited to: Kennesaw State University. Downloaded on October 13,2025 at 18:38:37 UTC from IEEE Xplore. Restrictions apply. critical advantage for security-sensitive applications. This aligns with the hypothesis that DP noise disrupts the stealthy gradient patterns required for persistent backdoor embedding, whereas EIFFeL's non differential privacy design remains vulnerable to such latent perturbations. The inverse relationship between main-task accuracy and backdoor suppression underscores a fundamental tension: privacy noise introduces targeted randomness that selectively degrades adversarial features more than legitimate learning signals.\nNotably, the cosine similarity validation mechanism (Fig. 5(b), (d), and (f)) demonstrates equivalent efficacy in both frameworks, with MPC-secured computation preserving detection fidelity despite DPSFL's noised gradients. This consistency confirms that the BR methods operate independently of the privacy layer under moderate attack intensities (30% malicious nodes), challenging the presumed incompatibility between DP and Byzantine resilience. However, DPSFL's 0.2% accuracy drop on FMNIST under coordinated additive noise attacks (Fig. 5(c)) hints at edge cases where DP perturbations may slightly amplify benign gradient misalignment-a trade-off warranting further theoretical analysis.\nThese results collectively demonstrate that integrating DP with established BR mechanisms achieves functionally equivalent robustness to non-private alternatives (EIFFeL) within the 30% adversarial threshold, while providing quantifiable privacy benefits. The framework's backdoor resistance further positions DP as a complementary defense layer against gradient-space attack vectors."
    },
    {
      "title": "Experimental Evaluation of System Overheads",
      "content": "The DPSFL framework was evaluated for end-to-end latency and model performance under varying user scales (|U |) and the number of secondary servers (|S|), with ε = 0.1 and Byzantine tolerance t = |S|/2 . Experiments on MNIST and CIFAR-10 (Fig. 6, Table II) reveal critical trade-offs between scalability and efficiency.\nFor MNIST (Fig. 6 Communication and accuracy metrics (Table II) further highlight task-specific sensitivities. MNIST maintains stable accuracy (97.12-98.12%) with modest communication growth These results establish practical guidelines: For lightweight tasks (e.g., MNIST), |S| = 20 balances latency (< 500s) and accuracy (> 98%). For complex tasks (e.g., CIFAR-10), limiting |S| ≤ 20 prevents verification overhead from dominating system latency (> 65%) while compensating convergence loss via dynamic local training adjustments. This empirically"
    },
    {
      "title": "Compare With Others",
      "content": "In Section II-C, we reference a related study introduced by Zhou et al. , which likewise proposes a framework integrating DP with BR. Subsequently, we delve deeper into comparing the advantages and disadvantages of our proposed scheme versus that of Zhou et al. through a series of experiments.\nTo enhance the efficiency of BR amidst noisy conditions, Zhou et al. put forth a novel BR strategy that combine normbased detection with accuracy-based detection. To elucidate the distinctions between our scheme and that of Zhou et al. , we incorporate their BR methodology within a MPC circuit. This integration poses several pivotal challenges:\nThe circuit must compute the accuracy of local models on a sample dataset, yet the parameters of these models are secret-shared, necessitating intricate handling of confidential information. ii) Unlike a straightforward filtering of anomalous models, the output of the circuit is intended to be the updated weight of each local model, further complicating the design. For experimental purposes, we consider MLP and CNN models as the analysis cases. Recognizing that fully connected layers correspond to vector calculations and that convolutional and pooling layers can be translated into linear operations simplifies the process. Moreover, a compact validation dataset is shared among all secondary servers, simplifying accuracy computations. Regarding the weights of local models, we adopt a twostep approach: initially calculating and subsequently disclosing them, followed by aggregation across all local models. To analyze the Byzantine robustness of DPSFL and the scheme proposed by Zhou et al. , we first consider a scenario where malicious users conduct label-flipping attacks while secondary servers perform cosine-similarity validation.\nThis experiment systematically evaluates the robustness of DPSFL and Zhou et al. 's method under varying malicious users ratios (0%, 20%, 40%, 60%) in federated learning. As shown in Table III, DPSFL consistently outperforms Zhou et al.'s method, particularly under high-adversarial conditions. On MNIST, DPSFL achieves 97.28% accuracy at 60% malicious users, surpassing Zhou et al. by 0.57%, while on CIFAR-10, the accuracy gap widens to 4.16% (68.75% vs. 64.59%), highlighting its effectiveness in complex scenarios.\nWe observe three key aspects emerging:\nProgressive Robustness Degradation: While both methods exhibit declining accuracy with increasing adversarial ratios, DPSFL maintains smaller accuracy drops. For MNIST, DPSFL's accuracy declines by 0.76% (98.04% → 97.28%) versus Zhou et al.'s 0.86% (97.57% → 96.71%). This trend amplifies for high-dimensional tasks: on CIFAR-10, DPSFL's 5.62% drop contrasts sharply with Zhou et al.'s 9.39% decline, underscoring its resilience to gradient-space attacks.\nTask-Specific Superiority: DPSFL excels in both simple and complex learning tasks. For FMNIST, it sustains > 90% accuracy even at 60% malicious users (90.41% vs. 89.71%), whereas Zhou et al. falters earlier, reflecting DPSFL's adaptive noise allocation and centralized Byzantine validation.\nBackdoor Resistance: The widening performance gap under higher adversarial ratios (e.g., +4.16% for CIFAR-10 at 60%) suggests DPSFL's fixed-parameter DP noise effectively disrupts adversarial model updates, while Zhou et al.'s dynamic noise adjustment introduces instability that exacerbates accuracy loss.\nThese results validate DPSFL's architectural advantages: its sequential validation-noise pipeline isolates Byzantine detection from DP perturbations, ensuring precise gradient filtering before noise injection. In contrast, Zhou et al.'s integrated approach struggles with mutual interference between BR and DP, leading to suboptimal equilibrium. The experiments further confirm DPSFL's scalability across heterogeneous tasks, positioning it as a robust solution for privacy-preserving federated learning in adversarial environments."
    },
    {
      "title": "CONCLUSION",
      "content": "In this paper, we introduced a novel federated learning framework that balances privacy protection with Byzantine robustness. By using secondary servers, our approach separates robustness checking from differential privacy noise injection. First, the system detects malicious local updates without interference; then, during aggregation, it adds verifiable DP noise to secure the data and ensure reliable model updates.\nThe key innovation is our noise-added Shamir secret sharing protocol, which incorporates (ε, δ)-DP noise into the sharing process. With cryptographic commitments and secure multiparty computation, this method overcomes the noise amplification issues found in traditional approaches. Experiments on MNIST, FMNIST, and CIFAR-10 show that the framework maintains nearly 98% of baseline accuracy for ε ≥ 0.1 and tolerates up to 60% malicious users, improving accuracy by 0.57-4.16% compared to existing methods. Moreover, the system scales linearly with the number of participants.\nOur framework's success is driven by three main ideas: 1) Using secondary servers to separately handle robustness evaluation and privacy protection. 2) Integrating MPC-based verification with Pedersen commitments to secure model updates and noise generation. 3) distributing the noise generation across multiple secondary servers, which lowers the noise burden on individual nodes while still meeting DP guarantees. This approach shows strong resistance against attacks such as label flipping, model poisoning, and backdoor injections, making it promising for applications in privacy-sensitive areas like healthcare and smart grids. Future work could investigate dynamic noise adjustment and hardware-accelerated verification to further enhance system reliability in adversarial environments."
    }
  ]
}