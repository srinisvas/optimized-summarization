{
  "title": "Press Release Ethics in AI: Performative Ethics in For-Profit AI Companies",
  "sections": [
    {
      "title": "Abstract",
      "content": "As various types of Artificial Intelligence (AI) enjoy a surge in popularity and funding, researchers, activists, and laypeople have questioned the associated negative consequences that use of AI technologies can bring. Sensitive to these concerns, many companies that specialize in various forms of AI development have published policies that describe the ethical considerations and protocols they have enacted in order to protect users and minimize societal harm that may be caused or exacerbated by the use of their products. While the policies are encouraging and seemingly comprehensive, multiple problems remain. Policy content is specific enough to encourage feelings of safety and comfort, but in many cases, it is still vague and amorphous enough to allow behavior that contradicts the initial reassurance. Companies reserve the right to change their policies as they deem appropriate, which can lead to users having outdated views on company stances. And in some cases, companies can deliberately renege on their own policies with impunity. As a result, companies can exploit users' ignorance or inattention and engage in problematic, \"irresponsible\" AI while maintaining a surface appearance of goodwill and ethical compliance. This critical analysis of certain AI companies' policies (and news coverage of subsequent issues and changes in those policies) aims to highlight examples of such pretense. The paper is limited to analysis of policies for 7 companies chosen for their name recognition in the AI landscape and news coverage: OpenAI, Google, X AI, Meta, Anthropic, Microsoft, and Anduril. AI companies' practice of making reassuring ethical claims in their policies then backtracking later on is labeled in this paper as \"Press release ethics."
    },
    {
      "title": "INTRODUCTION",
      "content": "The public is \"firmly wary\" about various forms of AI in their lives, and business people , economists , and Silicon Valley billionaires are dutifully offering reassurance about their AI technologies. Companies that specialize in various forms of AI development (\"CAIDs\") pledge to develop \"responsible AI\" , allowing \"organizations to make more ethical, effective and efficient decisions by eliminating potential sources of bias\" . Still, assurances that fundamental threats to our livelihoods and lives are very far down the line may ring hollow to the writers, programmers, and others whose jobs were replaced by AI within months of ChatGPT's release to the public in November 2022 . Indeed, it's not uncommon for companies in general to publicly say one thing and privately do another, sometimes simultaneously, as detailed in Section II.\nCAIDs show signs of engaging in the same bait-and-switch behavior when it comes to ethical concerns about their products and services. Given the specific new domain for this phenomenon, I posit a new term to describe such slippery, \"ethics washing\" practices: Press Release Ethics (PREs). Indeed, the ethical issues acknowledged and considerations offered by CAIDs make for a good press release: a genre of communication that is historically \"self-serving\" and allows CAIDs to focus on the positive despite undeniable harm caused by their products . Press releases enable companies to \"stress good news and downplay bad news\" , which CAIDs must do on a regular basis as the ethical concerns of AI adoption mount. The companies need to be seen as considerate of ethical concerns, but could fall behind their competitors if they were to fully embrace exclusively ethical practices, and thus are disinclined to do so from a business and, potentially, existential standpoint. PREs, in general, embody performative ethics. PREs, in the context of Responsible AI policies, are characterized by two phases. In Phase 1, companies make public ethical reassurances regarding AI applications (to appease the public). In phase 2, those companies quietly backtrack on those reassurances (to minimize harm to the company's development and profitability). I posit that many of CAIDs' purported policies are little more than ethics theater: public-facing moral principles and behind-the-scenes hypocrisy.\nIn addition to its obvious duplicity, this reassurance/backtracking combination in CAIDs brings its own additional ethical issues to bear. The high barrier to entry of the field of AI development, due to stratospheric demands for training data and computing power for the creation of AI programs, has resulted in researchers calling the domain \"Big AI\" . With CAIDs enjoying so much power (and, as will be described, so little oversight), there is an inherent power imbalance in the resulting landscape. The failure of companies to adhere to their own initial policies, instead changing them to suit evolving environments and profit goals or violating them outright, leaves ethical decision-making responsibility to third party groups and individual users (who aren't as well funded and whose concerns do not easily influence business strategies)."
    },
    {
      "title": "IRRESPONSIBLE AI",
      "content": "In recognition of the disruptive and potentially destructive power of AI technologies, disparate groups have called for such technologies to be used \"responsibly.\" That word emphasizes the duty that CAIDs have to exercise good judgment in the development and deployment of AI technologies given their positions of power over the general public. Responsible AI, however, means different things to different CAIDs. A suitable definition of the term for this paper is \"the framework and principles behind the design, development, and implementation of AI systems in a manner that benefits individuals, society, and businesses while reinforcing human centricity and societal value\" . The tenets of Responsible AI also vary across CAIDs, employees of which appear to be the authors of the most popular articles on the subject. Most CAIDs, however, claim adherence to a list of ethical considerations that is very similar to that of IBM's: \"accountability, reliability, inclusion, fairness, transparency, privacy, sustainability, and governance\" . This list is highly ambiguous. Even the more detailed explanations about each term (\"Inclusion aims to improve the usability and outcomes of machine learning models by ensuring a variety of racial, cultural, and experiential viewpoints are considered\" ) leave plenty to the imagination. How, exactly, is that variety of viewpoints \"considered\"? What weight is given to various \"viewpoints\"? The syntax and vocabulary words themselves are bland and vague, reminiscent of a chatbot's careful prose . The ambiguity of those tenets can certainly be attributed to a broad, layperson audience; not everyone concerned about AI can understand the technological processes needed to navigate, for example, IBM's impressive Fairness 360 toolkit which \"includes a comprehensive set of fairness metrics for datasets and models, explanations for these metrics, and algorithms to mitigate bias in datasets and models\" . The layperson-friendly ambiguity could also, however, simply be an easy out; companies drawing from \"a gallery of feel-good, Responsible AI technology remedies\" (as one a Forbes Councils Member put it in a call for Responsible AI) so that they have something to point to in order to placate uneasy users and regulators. Indeed, such bromides may provide just enough comfort for users to suppress their cognitive dissonance in using (and paying for) technologies that they find disconcerting . Regardless of the CAIDs' intentions in framing and phrasing their Responsible AI pledges and policies, discrepancies between policy assurances and problematic real-world applications fit the pattern of PREs.\n1 One indication of the company's intention to engage in the behavior they stringently denied is perhaps found in their 2018 patent application for an \"application module [that] record[s] ambient audio\" III. THE GREAT TECH COMPANY BACKTRACKS"
    },
    {
      "title": "For companies in general",
      "content": "Companies in general sometimes find themselves obligated to backpedal on policies that favor consumers and users when it is realized that the policy is too unfavorable for the company. People who become customers based on alluring promises can find themselves on the wrong end of a bait and switch when a company backtracks on its previous policy and installs a new one, less favorable to consumers and more favorable to the continued growth of company profits. When the clothing company L.L.Bean changed its decades-old no-time-limit, noreceipt-necessary return policy to one that allowed returns only within one year of purchase, customers balked, and some even sued the company . The Ford Motor Company put out a press release when it earned \"a perfect score\" on the \"Human Rights Campaign Corporate Equality Index, a national benchmarking survey and report on corporate policies and practices related to LGBT equality in the workplace\" , but withdrew its participation in the index a few years later when right-leaning critics voiced their displeasure of Ford's Diversity, Equity, and Inclusion (DEI) policies .\nOf course, companies may justify such reversals as inevitable as they respond to evolving market trends, as, according to a technology consultancy firm, \"even the longeststanding traditions and expectations can change overnight in an instant\" . If \"policies remain static, trapped in a time warp of sorts,\" as companies keep their original promises, they may not be able to \"mitigate liabilities and foster a secure operational landscape\" (those quotes are from a risk management company's blog article, which exemplifies an expedient use of chatbot prose itself). The business jargon belies the fact that companies draw in new customers by touting their attractive, but ultimately temporary, concerns for unwavering ethical practices, customer well-being, and fairness. Once customers are secured, the policy becomes subject to \"liabilities,\" and logically changes to suit the companies' bottom line. PREs are irrefutably profitable, and that profitability has historically outweighed, and continues to outweigh, concern for users and consumers. In fact, some researchers have concluded that \"[a]ny conversation concerned with the impact of AI and technology on society must also be concerned with the impacts of capitalism and the reality of economic profit over consumer impact\" , putting AI companies squarely in the space of general profit-centric businesses.\nTechnology companies are not exempt from such profitseeking, ethics back-tracking behavior. Google raised eyebrows when it removed the \"Don't be evil\" credo from the top of its company code of conduct several years ago . Meta, formerly Facebook, denied rumors of listening in on users via their cell phone microphones for the purpose of serving them tailored ads multiple times since at least 2017 , only for their advertisers to admit to the practice in 2024 1 . Video telephony platform Zoom was charged by the FTC with misleading users \"since at least 2016… Zoom [touted] that it offered 'end-to-end, 256-bit encryption' to secure users' communications, when in fact it provided a lower level of security\" . The company also made changes to its terms in 2023, quietly mentioning in the updated version that the company would now reserve the right to use customers' \"video recordings, audio transcripts, or shared files… for lots of things, including training Zoom's machine learning and artificial intelligence applications\" . To summarize the issue, While each of the mega -companies, at least on paper, boasts a set of values unquestionable for their integrity, it is also true that most have had to face crises precisely due to the lack of some of the principles they themselves proclaim .\nSimilar to the ethically dynamic nature of company documentation, there is a personnel component to PREs. Some ethics-minded employees even find themselves having to permanently sever ties with the company due to blowback, as in the case of former Google researcher Timnit Gebru . Facebook whistleblower Frances Haugen noted that Meta \"chooses profits over safety\" in documents provided to journalists before her departure from the company . The performative aspect of Silicon Valley PREs is now on full display, but arguably technology companies including Meta have successfully navigated the blowback associated with the behind-the-scenes renunciation of their own purported concern for user well-being. Even when there is negative press coverage of such events, there doesn't seem to be much effect on the companies' continued operations or bottom lines , emphasizing both to the public and to the companies themselves that no change to their modus operandi will be demanded. This convenient repudiation of ethical tenets is now being repeated in CAIDs."
    },
    {
      "title": "For CAIDs in particular",
      "content": "Certainly, CAIDs are not immune to the evolving market trends which, historically, have forced companies to relegate ethics to the backburner despite previous claims of adherence to Responsible AI policies. But the principles of technology ethics, such as the assertions that \"health, safety and welfare of the public is primary\" and that software should be approved only if [programmers] have a well-founded belief that it is safe, meets specifications, passes appropriate tests, and does not diminish quality of life, diminish privacy or harm the environment. The ultimate effect of the work should be to the public good\" , are categorized as \"fundamental\" and certainly should not change with market trends. Yet CAIDs arguably not only break from moral codes in their business practices, but also break their own policies as the winds shift. Investigative reporting from the New York Times showed that \"OpenAI, Google and Meta 2 Relying on CAIDs to regulate themselves is unsurprisingly problematic, but their reaction to government regulation is also difficult to follow. Some CAIDs have publicly supported statelevel legislation in California that puts limits on their ability to train their models, while others have opposed it . The legislation has been criticized both for not going far enough in its limitations of CAIDs and for going too far and \"stifling innovation\" according to OpenAI .\nThere is no federal level legislation regulating CAIDs at the time of this writing .\nignored corporate policies, altered their own rules and discussed skirting copyright law as they sought online information to train their newest artificial intelligence systems\" . It seems that CAIDs are ready to adhere to publicly declared Responsible AI tenets until it's no longer profitable to do so -a clear match to the description of PREs.\nAround the end of 2022, multiple CAIDs united in a public call for a moratorium on AI development , but the pause never quite materialized : another indication of the variance between CAIDs stated plans and their own actions 2 . Individual CAID safety teams also seem to suffer startling rates of attrition, and in cases total disbandment, which will be discussed in detail in the next section."
    },
    {
      "title": "Open AI",
      "content": "The formation of \"safety teams\" at OpenAI was heralded with press coverage and touted on the company website . The original team was disbanded, however, after less than one year in existence , and replaced by one headed by its own CEO 3 . Arguably, the precariousness of safety teams at OpenAI is due in part to \"a culture of recklessness and secrecy\" as one former employee described . Journalists have struggled to find former OpenAI employees who were willing to share their experiences due to some punitive protocols for those leaving the company:\nThat's partly because OpenAI is known for getting its workers to sign offboarding agreements with non-disparagement provisions upon leaving. If you refuse to sign one, you give up your equity in the company, which means you potentially lose out on millions of dollars. Still, with some current employees speaking anonymously and some former employees forgoing the offboarding agreement, there are records of people with knowledge of the company's inner workings reporting concerns that do not align with the upbeat assurances we see in the OpenAI safety policy. Daniel Kokotajlo is \"a former researcher in OpenAI's governance division\" who said in an interview \"that the probability that advanced A.I. will destroy or catastrophically harm humanity… is 70 percent\" . The alignment of OpenAI's safety practices with the characterization of PREs is also evident in consideration of the former researcher's description of a disconnect between company safety protocols and actions: \"At OpenAI, Mr. Kokotajlo saw that even though the company had safety protocols in place … they rarely seemed to slow anything down\" .\nBeyond the uses of such technologies, their creation is also facilitated by duplicitous practices, including scraping training data from unapproved sources. OpenAI created speech to text programs so that they could feed the text from YouTube video transcriptions into their LLMs. Not even Google, though, could bring themselves to call OpenAI out on this practice. Why not? Some Google employees were aware that OpenAI had harvested YouTube videos for data, two people with knowledge of the companies said. But they didn't stop OpenAI because Google had also used transcripts of YouTube videos to train its A.I. models, the people said. That practice may have violated the copyrights of YouTube creators. So if Google made a fuss about OpenAI, there might be a public outcry against its own methods, the people said. CAIDs, then, cannot call out unethical behavior or uses of their datasets by other companies because they themselves may be engaged in the same secretive and duplicitous practices. As such, phase 2 of PREs (that is, the phase in which the company backpedals on its previous declarations of ethical behavior) are understandably played down. Only investigative journalism and research can bring the actual business practices to light, since the CAIDs themselves maintain the charade of adhering to their own policies. 42) Google (Gemini): In an open letter introducing Google's AI assistant, Gemini, Google CEO Sundar Pichai wrote, \"We're approaching this work boldly and responsibly\" and alluded the company's AI principles, which state that Google's objectives for AI applications will \"[b]e socially beneficial, be built and tested for safety, be accountable to people\" and that \"we will not design or deploy AI [that is applied in the area of t]echnologies that cause or are likely to cause overall harm . The company goes beyond what most of its AI competitors provide in the way of documentation of changes to policies, providing a list of updates and iterations of its policies in easily accessible PDF form. When initial versions of Gemini were documented to generate images with historical inaccuracies and racial bias , the program was suspended and the senior vice president of the company published an open letter entitled \"Gemini image generation got it wrong. We'll do better\" . The overall impression when reading the company's Gemini introduction and AI Principles is one of integrity: taking responsibility for its formidable computing power and full disclosure of its shortfalls.\nBut beyond the company's own documentation, PRE attributes are evident. For example, it was found that \"Google violated its promised standards\" in placing third-party advertisements , and since the company uses AI for advertising 5 , this violation fits comfortably in the AI PRE category: Google promised one thing in its AI policies but did another, accepting payments from customers who thought the company was following its own rules. Summarizing a logical response to such PREs, one customer said \"'I feel cheated… What I requested to buy was not what I got'\" .\nThe size needed for viable (and marketable) LLM training sets has also led CAIDs, including Google, to reconsider its previous guarantees of privacy and protection for users. Those size requirements have already resulted in CAIDs exhausting publicly available data online and bumping up against paywalled content in their search for more data to feed their programs . I have already discussed OpenAI and Google scraping user content from the open internet in the form of YouTube videos. Google has an advantage in the realm of information beyond the open internet, of course. Millions of people use its Drive services to create their own documents, and its review platform also contains plenty of user-generated content. The company has now changed its terms of service (after the vast majority of users had agreed to previous versions with different clauses) to enable it to harvest those sources of personal, private information for the purpose of training its LLMs:\nLast year, Google also broadened its terms of service. One motivation for the change, according to members of the company's privacy team and an internal message … was to allow Google to be able to tap publicly available Google Docs, restaurant reviews on Google Maps and other online material for more of its A.I. products. \"Tapping\" information sources beyond the scope of their initial protocols is a glaring example of secondary use, or the use of data for reasons beyond their primary purpose. In this case, Google markets (and profits from) its Drive services as allowing users to easily generate documents for themselves, an arrangement that has drawn customers since 2012 . Only recently has the company maneuvered so that it can now also harvest its customers' documents to train AI models, constituting secondary use, the exclusive benefit of which is derived by the company.\nIn chorus with other CAIDs, Google has also suffered attrition and full disbandment of their AI safety team, the \"Responsible Innovation team\" . Also in chorus with many CAIDs, the company has publicly declared that despite the safety team disbanding, safety is a top priority for the company: \"Despite these changes, a Google spokesperson assured that the team's mission will proceed in an even more robust manner, though specifics were not disclosed\" . It is the lack of specifics, of course, that are emblematic of PREs: the overarching public message is one of reassurance, but details don't bear that out.\nPerhaps most egregiously, Google quietly changed course on its pledge to develop AI for responsible uses, with a gentle notice at the top of its 2018 blog post about responsible AI: \"We've made updates to our AI Principles.\" Those updates?\nThe company removed language promising not to pursue \"technologies that cause or are likely to cause overall harm,\" \"weapons or other technologies whose principal purpose or implementation is to cause or directly facilitate injury to people,\" \"technologies that gather or use information for surveillance violating internationally accepted norms,\" and \"technologies whose purpose contravenes widely accepted principles of international law and human rights.\" The adherence to PRE methods is undeniably apparent in this removal."
    },
    {
      "title": "Grok-2 (via xAI:)",
      "content": "The social media company X, run by Elon Musk, has also entered the generative AI space with Grok (and now Grok-2). Musk has a rather negative record when it comes to business ethics: \"Elon Musk has violated a lot of the social contract, and its basis of trust, with employees, with investors, suppliers, regulators, and other parts of his ecosystem\" . Specifically, the incentivization of violent content on X under Musk has drawn extensive criticism , and he has been charged with fraud by the Securities and Exchange commission . The CEO doesn't sugarcoat his views on the damaging potential of AI: \"There is some chance that is above zero that AI will kill us all… I think we should also consider the fragility of human civilization\" . Given this fairly negative public perception, it is tempting to spare Grok-2 from the list of PRE CAIDs. But despite a CEO with plenty of bad press, there are still vestiges of performative ethical assurances scattered throughout the company's documentation.\nIn the Grok-2 \"About\" page, for example, it is claimed that the CAIDs goals are for the common good: \"xAI is a company working on building artificial intelligence to accelerate human scientific discovery. We are guided by our mission to advance our collective understanding of the universe.\" The Acceptable Use Policy for Grok-2, composed of only 248 words including the title and date, concludes with a phrase that can only be interpreted as its writers claiming the high moral ground: \"Be a good human, it's really not that hard\" .\nAs such, there is indeed the hallmark discrepancy between stated company policy and documented company action. \"Unlike more restrained AI models like ChatGPT or Google's Gemini, Grok-2 seems to operate with fewer ethical guardrails\" . Grok-2, born as it is from a social media company, joins Meta as a CAID that has direct influence in global discourse. Rather than using that influence to attempt to slow the waves of online misinformation that plague society today, AI models like Grok-2 instead add to the chaos: \"In an era when distinguishing fact from fiction online is already challenging, tools like Grok-2 could exacerbate the spread of misinformation and deepen societal divisions\" . Of course, X and Grok-2 are not the only social media sites that profit from users' engagement with manufactured content and the calcification of viewpoints in algorithm-powered filter bubbles. Other social media companies, however, at least go through the motions of content moderation. \"Grok allows misinformation to proliferate unchecked, a significant departure from the moderated environments that its competitors maintain\" ."
    },
    {
      "title": "Meta AI",
      "content": "As one of the most successful social networking platforms in existence, Meta (a parent company that now owns Facebook, Instagram, and Whatsapp) has substantial name recognition when it comes to dubious ethical practices.\nResearch documenting problematic business practices at the company goes back decades and continues to this day.\nMeta's ethical messaging is, predictably, heavy on optimistic rhetoric and world improvement, from its introduction (\"we build products and experiences to give people the power to build community and bring the world closer together\") to the individual principles (\"Keep people safe and protect privacy-we are committed to protecting our communities from harm\") . But true to PRE form, these policies are more aspirational than concrete. Meta founder Mark Zuckerberg has testified at governmental hearings that were called in response to legislators' alarm at the company's lucrative algorithms and the myriad societal ills that came as a direct result: the proliferation of misinformation, societal division, and even child exploitation, among others . It is widely acknowledged that the company's ethical failures are, to borrow from programmer Sandra Lee Harris' 1971 user manual , a feature, not a bug: \"If Facebook employed a business model focused on efficiently providing accurate information and diverse views, rather than addicting users to highly engaging content within an echo chamber, the algorithmic outcomes would be very different\" .\nAs Meta joins the CAID space, the company is further engaging in PRE practices. The discrepancy between company policy and business practice are clear. \"Privacy-focused\" messaging platform WhatsApp shares users' data with Facebook and gradually degrades if users do not agree to that policy . Zuckerberg is also laying the foundation to abdicate ethical responsibility from his own company to the U.S. government when it comes to controlling the AI technologies from which he profits: \"Congress should engage with AI to support innovation and safeguards. This is an emerging technology, there are important equities to balance here, and the government is ultimately responsible for that\" .\nWhile it seems that Zuckerberg believes Meta's limitations should come from Congress, not its own policies, company lawyers reportedly do not shy away from lawsuits that may result from sidestepping government regulations. In its quest to harvest training data for its AI programs, the company appears ready to buy out obstacles in its path and to break some rules in the name of expediency: At Meta, which owns Facebook and Instagram, managers, lawyers and engineers last year discussed buying the publishing house Simon & Schuster to procure long works, according to recordings of internal meetings obtained by The Times. They also conferred on gathering copyrighted data from across the internet, even if that meant facing lawsuits. Negotiating licenses with publishers, artists, musicians and the news industry would take too long, they said. Given the discrepancies between Meta's own policies and their past and current behavior, it is reasonable to conclude that they will fit squarely into the PRE category among CAIDs. 6New Zealand, and Australia. In doing so they made Llama open source, a move that quickly resulted in its use by the Chinese military , ."
    },
    {
      "title": "Anthropic (Claude)",
      "content": "The company that developed the Claude AI assistant, Anthropic, has cultivated a reputation of ethical behavior amidst rival AI giants . Its founders all started at OpenAI, but left to create their own AI company \"on a promise of building reliable and steerable AI systems\" . When more employees and executives left OpenAI due to ethical concerns (documented in the previous section), several joined Anthropic , with the implied message that Anthropic did not share the dismissive attitude toward ethics and safety that OpenAI had. Indeed, Anthropic researchers have identified and publicized shortcomings of their LLMs , and the company co-president has been profiled in The Atlantic in an article entitled \"Building AI With a Conscience\" . The company declares on their help center website that \"User safety is core to Anthropic's mission of creating reliable, interpretable, and steerable AI systems\" , and their AI Constitution details an approach to training models while \"helping to avoid toxic or discriminatory outputs, avoiding helping a human engage in illegal or unethical activities, and broadly creating an AI system that is helpful, honest, and harmless\" .\nBut a closer look reveals some hallmarks of PREs even in this safety-focused company. Some subtle gray-on-white text at the top of The Atlantic article indicates that it is \"Sponsor Content\" rather than a feature created by the editorial staff 7 . Anthropic accepted a multi-billion-dollar investment from Google, resulting in an antitrust investigation in the United Kingdom . Web publishers have bristled at the company's training methods, relating that \"Anthropic is swarming their sites and ignoring their instructions to stop collecting their content to train its model\" . The contradictions between this CAID's stated practices and its real-world practices have led journalists to ask: \"Is it even possible to run an AI company that advances the state of the art while also truly prioritizing ethics and safety?\" and ultimately answer in the negative, concluding that \"even high-minded Anthropic is becoming an object lesson in that impossibility\" ."
    },
    {
      "title": "Microsoft (Copilot)",
      "content": "Like most CAIDs, Microsoft extensively documents its commitment to ethical business practices on its consumer-facing site:\nOur commitment to corporate responsibility and integrity guides everything we do as a company and defines the work of our ethics and compliance program. We have high ethical standards governing the way we conduct our business, standards that we also apply to our suppliers and business partners. Our business practices and standards reflect our commitment to making a positive impact around the globe. We demand such high standards from ourselves and our partners to preserve trust with our customers, governments, investors, partners, representatives, and each other, and because it is the right thing to do.\nAdditionally, on the company's \"Trust Code\" documentation for employees (linked to from the user-facing page), a large, bold font subheading instructs: \"When making decisions, ask 7 \"This content is made possible by our sponsor and is independent of The Atlantic's editorial staff\" yourself: does this build or harm trust with our customers?\" There are 53 pages in the Trust Code document, detailing the company's supposed commitment to earning trust with customers, governments, communities, investors, the public, suppliers, and employees. All told, there are dozens of pages of company documentation dedicated to the message that Microsoft is an ethical, trustworthy company. As another researcher summarizes: \"Overall, Microsoft's AI principles reflect a comprehensive and thorough approach to the development and use of their own technology\" .\nAs with other CAIDs, some of those claims appear to be aspirational rather than de facto. Researchers have called the company out for unethical business practices for decades, documenting issues from unfair competitive practices to enabling censorship by complying with Chinese requirements for doing business . In the AI realm, in 2023, the company laid off its entire ethics and society team within the artificial intelligence organization… [leaving] Microsoft without a dedicated team to ensure its AI principles are closely tied to product design at a time when the company is leading the charge to make AI tools available to the mainstream\" A software engineer at the company reported that Microsoft's AI image generator \"created … sexualized images of women in violent tableaus, and underage drinking and drug use\" . While it arguably takes some trial and error to discover and fix such issues, the problem for Microsoft appears to be at the structural level. Firing the ethics and society team, argue some ethicists, resulted in issues, which should have been fixed prior to release, being released into the world in real time: \"Microsoft is in deep trouble because of the model they've adopted. And the ethicists who are pulling the whistleblower siren have to do so because they got rid of the people inside who would help them\" .\nAnduril: Anduril, a company that makes autonomous weapons (uniting AI and warfare), meets the qualifications for PREs. A journalist interviewing one of the company's co founders and former Trump military advisor, Trae Stephens, summed up the ethical bait-and-switch in his questioning: \"When I wrote about Anduril in 2018, the company explicitly said it wouldn't build lethal weapons. Now you are building fighter planes, underwater drones, and other deadly weapons of war. Why did you make that pivot?\" . The cofounder defended his company's ethical flip-flop with vague business jargon and xenophobic fear-mongering, and continued to claim that the company is standing by their ethical principles in creating machines that kill human beings:\nWe responded to what we saw, not only inside our military but also across the world. We want to be aligned with delivering the best capabilities in the most ethical way possible. The alternative is that someone's going to do that anyway, and we believe that we can do that best. \"We can do that best\" is a slogan worthy of a press release. By invoking that phrase as he gives reasons why, six years ago, he said the company would do the opposite of what it's doing today, Stephens gave a prime example of PREs.\nStephens also echoed Zuckerberg's previously documented abdication of ethical responsibility, concluding that the government should set ethical boundaries rather than companies themselves: \"I don't think that there's a whole lot of utility in trying to set our own line when the government is actually setting that line\" . His fellow co-founder Palmer Luckey agreed, stating \"I don't think I'm the guy to teach people ethics. I can give people my perspective\" 8 .\nLater in the interview, Stephens pushed the limits of performative ethics, declaring that not only is his autonomous weapons company doing the right thing, but that he wishes other technology companies would be more ethical, going so far as to claim the ultimate divine ethical endorsement: \"The call that I have been trying to make to the tech community is that we have a moral obligation to do things to benefit humanity, to draw us closer to God's plan for his people\" .\nAnduril does not have a published ethics code to contradict. I posit, however, that the company still qualifies as engaging in PREs due to its co-founders going back on their word about developing autonomous weapons in the first place, their hypocrisy in suggesting that other technology companies have scorned Anduril's pleas for better moral behavior, and their blasphemous audacity in declaring that they are enacting God's plan by profiting from the creation of autonomous killing machines."
    },
    {
      "title": "PERFORMATIVE ETHICS IN ACADEMIA AND RESEARCH",
      "content": "Academic research has also been accused of such performative ethics and cannot be excluded from this conversation. While researchers may consider themselves to be neutral parties in the documentation and expansion of AI ethics, their lack of introspection and self-analysis is performative and potentially destructive: \"Without systemic analysis… work dedicated to positively improving the impact of technology on society will be performative at best and reify systems of oppression at worst\" . Critical theory scholars have identified systems of oppression in myriad places in society, and now those systems are manifesting in a post-AI world.\nIt is easy to see how systems of oppression may manifest in AI companies: CAIDs oppress regular users by training systems on their content without notice or consent , , and have used the resulting technology to render users' jobs obsolete and even kill people . While CAIDs profit and grow from that oppression, regular users are encouraged to adapt to the situation . The capitalistic component of our society seems to have convinced regular people that they must use (i.e., pay subscription fees to use) AI in order to remain employable in a job market in which that same AI threatens their employability. This paradox, which greatly favors CAIDs at the expense of regular people, is indicative of the creeping normalcy phenomenon:\n… the term \"creeping normalcy\" [can] refer to such slow trends concealed within noisy fluctuations. If the economy, schools, traffic congestion, or anything else is deteriorating only slowly, it's difficult to recognize that each successive year is on the average slightly worse than the year before, so one's baseline standard for what constitutes \"normalcy\" shifts gradually and imperceptibly. It may take a few decades of a long sequence of such slight year-toyear changes before people realize, with a jolt, that conditions used to be much better several decades ago, and that what is accepted as normalcy has crept downwards. Indeed, even in the relatively quick-paced explosion of the use of certain types of AI, CAIDs have established the normalcy, and inevitability, of their domination in society.\nAs mentioned in the introduction, due to the vast amounts of training data and computing power needed to develop AI, researchers have classified CAIDs as \"Big AI.\" Smaller and slower moving companies (and perhaps those that pause development in order to ensure ethical goals are met) are simply shut out. This setup contextualizes the CAID sentiment that AI that is developed fast is the most profitable, an idea summarized by Michael Woolridge of Oxford. In an article he wrote about Big AI, Woolridge theorized about the inner monologue of a CAID leader, writing \"the race for scale that we have witnessed in AI over the past five years is perhaps no surprise: if bigger is better, then let us make it bigger-and let us do it before our competitors\" . Hence the importance of groups of resistance: without researchers, advocacy groups, and the government reining in CAIDs, only capitalism and its mantra of constant growth will control CAIDs, which of course means no control at all. Their dominance will continue to breed more success, contributing to a status quo in which ethical concerns are effectively performative: lines in a script that do nothing to control or mitigate profitable harm on a large scale. The status quo is a powerful force in upholding systems of oppression , within tech fields and without.\nResearchers have documented user trust in big companies, showing that many users use websites and applications without reading the terms and conditions because they believe that \"If there were anything bad in the policy, it would be illegal\" . But governmental regulation is far outpaced by CAID developments, and there is still no enforceable law at the federal level that regulates CAIDs. Instead, there is a patchwork set of state laws (in Oregon, Montana, New Hampshire, Tennessee, and Delaware) and national blueprints that indicate an understanding of the issue but not a concrete way to address it or punish violations at the federal level.\nWith CAIDs being profit-motivated, consumers feeling the need to adapt to CAIDs, and the U.S. government lagging in enforceable legislation to temper unethical AI practices, it is arguably researchers who are best placed to identify and interrogate problematic AI ethics policies and actions. I add another consideration to McFadden and Alvarez's call for researcher critical theory: research that addresses ethics in AI without interrogating the adjacent PREs component contributes to the tacit acceptance, and subsequent perpetuation, of historical systems of oppression. Research of CAID PREs without denouncement adds to the creeping normalcy mentioned previously, and contributes to inertia bias: When the first wave of users accepts the discrepancies between CAIDs stated policies and their actual practices, the acceptance becomes entrenched and harder to resist for future generations.\nCONCLUSION: OPTIMISM AND HEALTHY SKEPTICISM Subjecting new AI technologies to critical analysis does not stop their exponential rate of development and adoption in the marketplace. As described in section II, even the CAIDs themselves find controlling or decelerating AI development an impossible task. This paper focused on seven CAIDs: OpenAI, Google, X AI, Meta, Anthropic, Microsoft, and Anduril. That focus should not, however, be interpreted to mean that other CAIDs don't exist or don't have problematic PREs in their policies.\nEvolving landscapes are, of course, inevitable as people use brand-new and untested technologies, including those that use AI. People and companies may be aspirational and have to regroup when they realize their stated goals are too optimistic and not 100% compatible with the current reality. The fact that CAIDs and other businesses fall short in similar ways may be part of the human experience. The medical breakthroughs alone of AI use demonstrate that a complete shutdown of this new technology would be damaging to humans.\nBut given the power, scale, and amounts of money flowing into the sector, now is the time to be brutally honest about what CAIDs do. There are glaring discrepancies between CAID companies' policies and their actual practices, ranging in severity from copyright violations to killing human beings. Researchers must amplify their concerns about the ethical challenges of these systems and companies. Teachers must resist calls to implement AI in their classrooms without a healthy dose of interrogation, and perhaps even reject those calls altogether in order to ensure that students do not blindly adopt such tools for the everyday challenges they face. Governments should regulate CAIDs heavily. CAIDs themselves must abandon PREs and instead make their business practices match their policies (and vice versa).\nWith governments slow to develop legislative limits to CAID power, and CAIDs themselves insisting that their only limits should come from governmental regulation, researchers and users may be the best advocates for change. As matters stand, the most urgent primary need for researchers and users is a strong sense of cynicism when reading ethics statements and policies from CAIDs. Given the volatile relationship between profit-driven business practices and ethical concerns, we cannot take CAIDs at their word until there is a dramatic shift in accountability for the industry."
    }
  ]
}