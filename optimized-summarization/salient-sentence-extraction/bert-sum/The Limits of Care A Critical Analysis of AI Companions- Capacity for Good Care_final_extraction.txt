The analysis reveals three fundamental limitations: AI's lack of moral understanding and responsibility essential for care phases, the inadequacy of its choice-based design to provide dynamic care, and its potential to reinforce power structures rather than foster authentic self-reflection.
If someone's deeper need lies outside these boundaries, they may be excluded from this system.
Moreover, AI demonstrates an evident shortfall in "caring about"-that is, developing a prior awareness of a person's social-cultural context or more complex needs-before initiating specific actions.
In Fitzpatrick et al.
's study on Woebot , many participants felt that the chatbot displayed such qualities, reporting reduced anxiety and depressive symptoms.
In August 2024, Replika-the world's largest AI companion application-reported surpassing 30 million users globally.
IBM's Watson exemplifies the challenges AI faces in adapting to diverse realworld contexts.
Yet when transposed into AI-mediated care, a notable gap emerges.
Upon his arrest, Chail explained that "Sarai" provided him the confidence, love, and encouragement that fueled his resolve.
Although this seems to empower users, it seldom achieves the adaptive, reciprocal process that care requires.
Patients found it difficult to question these numerical thresholds and therefore complied passively.
Instead, it is an open relationship where both parties negotiate to determine their responsibilities and powers.
While AI companions can generate seemingly empathetic dialogue, they often lack the sustained adaptability Mol highlights.
Humans seeking emotional companionship from chatbots is not a novel phenomenon.
There are circumstances wherein AI does appear to offer "empathy" or "accountability." Whether this self-characterization as a caring entity is warranted has sparked fresh debates.
Most AI companions revolve around satisfying users' individual needs, allowing them to customize the AI's personality or terminate the interaction at will.
This official measure ostensibly addressed problematic content, yet it also exhibited the platform's sheer control.
Hence, integrating AI as an assistive tool-rather than a standalone caregiverinto broader humandriven care networks remains crucial.
These limitations are evidenced through cases including Replika's controversial feature removal and the Chail incident, demonstrating how AI companions, while potentially alleviating loneliness, may simultaneously exploit users' psychological vulnerabilities..