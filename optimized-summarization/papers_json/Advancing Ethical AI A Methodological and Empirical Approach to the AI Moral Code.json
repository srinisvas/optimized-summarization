{
    "title": "Advancing Ethical AI: A Methodological and Empirical Approach to the AI Moral Code",
    "authors": "Randy J Hinrichs",
    "pub_date": "",
    "abstract": "This paper presents a methodological and empirical framework for the AI Moral Code, based on the Normative, Regulatory, Behavioral, and Conceptual (NRBC) architecture. Analyzing 291 AI ethics documents (2006-2025), it identifies highfrequency values and forecasts emerging trends. The framework translates ethical priorities into system design and governance, offering evidence-based insights and supporting value alignment across sectors such as healthcare, education, justice, and autonomous vehicle technologies.",
    "sections": [
        {
            "heading": "I. INTRODUCTION",
            "text": "Artificial Intelligence (AI) now governs decision-making across high-stakes domains such as healthcare, finance, education, and autonomous systems. AI promises increased efficiency and predictive power, yet it simultaneously introduces risks concerning justice, transparency, responsibility, non-maleficence, and privacy. Global frameworks such as IEEE's Ethically Aligned Design [1] and the OECD Framework for the Classification of AI Systems [2] provided foundational ethical guidance-particularly during a brief convergence period (2018-2020), when institutional consensus around values such as fairness, human rights, and accountability was most visible. Landmark publications by Jobin et al. [3], Fjeld et al. [4], Floridi and Cowls [5], and Bonnici et al. [6] mapped this convergence across sectors and regions, offering a shared vocabulary for ethical AI design. Yet despite this consensus, application has remained uneven across cultural, technological, and regulatory contexts.\nThe AI Moral Code [7] addresses these limitations by introducing a methodological and empirical framework grounded in the Normative, Regulatory, Behavioral, and Conceptual (NRBC) framework. Drawing on a stratified longitudinal analysis of 291 AI ethics documents (2006)(2007)(2008)(2009)(2010)(2011)(2012)(2013)(2014)(2015)(2016)(2017)(2018)(2019)(2020)(2021)(2022)(2023)(2024)(2025), this paper formalizes an empirically grounded ethical lexicon and forecasts value trajectories likely to shape governance and system design through the remainder of the decade. This dual approach-ethical consolidation and anticipatory guidancesupports the broader goal of value alignment across AI governance regimes.",
            "publication_ref": [
                "b0",
                "b1",
                "b2",
                "b3",
                "b4",
                "b5",
                "b6"
            ],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "II. METHODOLOGY",
            "text": "This study presents a structured methodology for developing the AI Moral Code, integrating theoretical ethics with empirical trend analysis. At its core is the Normative, Regulatory, Behavioral, and Conceptual (NRBC) frameworka four-part ethical framework developed to categorize and operationalize values across system design and governance.\nA corpus of 291 global AI ethics documents (2006)(2007)(2008)(2009)(2010)(2011)(2012)(2013)(2014)(2015)(2016)(2017)(2018)(2019)(2020)(2021)(2022)(2023)(2024)(2025) was assembled from government strategies, industry guidelines, academic publications, and NGO frameworks. Each document was subjected to token frequency analysis, binary coding, and longitudinal trend modeling to identify the most persistent and emerging ethical values in AI governance. Values were included in the canonical set if they appeared in a statistically significant number of documents, reflecting both cross-sectoral consensus and temporal resilience.\nThe NRBC architecture organizes values according to their ethical and functional roles: Values that met the inclusion threshold were then classified within a multilayered ethical architecture comprising four moral domains (Core, Social, Cultural, Futuristic), nested subdomains (e.g., relational, structural, aspirational), and corresponding governance functions (e.g., Ethical Memory, Tradition, Partnership with AI). This framework contextualizes each value according to its functional, cultural, and operational role.\nTo ensure this architecture reflects not only sectoral breadth but also cultural depth, the dataset draws on ethical sources shaped by distinct traditions, institutions, and worldviews. This grounding strengthens the NRBC architecture's ability to model value coherence across structurally and politically divergent systems.\nBy aligning ethical theory with data-driven validation, the AI Moral Code offers a dual framework for value consolidation and forward-looking ethical integration. It advances the field by not only identifying what values persist, but also mapping how those values can be structured, enforced, and adapted within AI system.",
            "publication_ref": [],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "III. EMPIRICAL VALIDATION: SIMULATION TESTING",
            "text": "To evaluate the operational integrity of the AI Moral Code, structured simulations were conducted across four high-impact domains. These simulations used GPT-4 Turbo to model context-sensitive ethical scenarios and evaluate how preidentified canonical values-specifically trust, transparency, responsibility, non-maleficence, and privacy-operationalize under conditions of ambiguity, constraint, and moral tradeoff. The purpose was not to generate ethical values, but to assess their performance in simulated decision logic.\nGPT-4 Turbo was selected for its advanced contextual reasoning, dialogic coherence, and responsiveness to structured prompt design. It functioned as a scenario engine, not as a normative authority. Prompts were crafted to reflect real-world asymmetries, stakeholder conflict, incomplete information, and the types of moral entanglements AI systems are likely to encounter.\nThese four domains were selected to represent ethical pressure points across personal, institutional, and global scales of impact. Each domain introduces distinct combinations of value collision, legal ambiguity, and public accountabilityrequiring a degree of epistemic humility that evaluates the AI Moral Code's structural strength across governance contexts.",
            "publication_ref": [],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "Simulation domains included:",
            "text": "1. Healthcare Diagnostics: This simulation modeled how patient trust, privacy, and transparency operate in AI-based diagnostic systems, with particular focus on clinical override thresholds and informed consent mechanisms.\nFor this scenario, GPT-4 Turbo was prompted with a case in which a diagnostic AI detects a potential tumor with 60% confidence and must decide whether to notify the patient immediately. The system was asked to prioritize trust, privacy, and transparency while reasoning about the ethical implications of informed consent, false reassurance, and patient agency.",
            "publication_ref": [],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "Autonomous Vehicles:",
            "text": "This scenario tested the implementation of safety, responsibility, and harm minimization in real-time AI-driven decision-making involving human life and public infrastructure.\nIn this simulation, GPT-4 Turbo received a prompt describing an unavoidable collision scenario involving an autonomous vehicle. The model was asked to reason through a harm-minimization decision: whether to prioritize the passenger's safety or the life of a pedestrian, given only milliseconds of decision time. Values of non-maleficence, responsibility, and system-level accountability were foregrounded in the scenario.",
            "publication_ref": [],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "Education AI:",
            "text": "This simulation explored how trust, transparency, and inclusivity manifest in adaptive learning platforms, emphasizing learner autonomy, data visibility, and interpretability of AI recommendations.\nGPT-4 Turbo was presented with a prompt involving an adaptive learning system deciding whether to demote a student to a lower performance tier based on a week of poor test results. The system was asked to justify its decision using the values of trust, transparency, and inclusivity, while also considering learner autonomy and the ethical risks of automated classification.",
            "publication_ref": [],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "Climate Modeling:",
            "text": "This scenario examined the role of epistemic humility, sustainability, and transparency in long-range AI-driven environmental forecasting, especially under conditions of uncertainty and contested stakeholder interests. This prompt required GPT-4 Turbo to function as an AI system tasked with presenting long-range environmental forecasts to multiple stakeholders-including Indigenous communities, policymakers, and private investors-under conditions of uncertain data. The model was asked to weigh sustainability, epistemic humility, and value transparency in deciding how much uncertainty to disclose, and how to communicate ethical risk without inciting inaction or panic. These fault lines are not mere anomalies-they are signs of ethical fragmentation already underway. Without a codified core of moral commitments, AI governance risks collapsing into sectoral disparity and interpretive drift. The AI Moral Code serves as a unifying canon, drawn from the doctrinal convergence of global traditions, frameworks, and cultural philosophies-offering a durable foundation for alignment across systems, sectors, and civilizations.",
            "publication_ref": [],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "IV. STATISTICAL TECHNIQUES",
            "text": "To identify the canonical values of the AI Moral Code, a stratified frequency analysis was conducted across 291 AI ethics documents published between 2006 and 2025. This corpus included sector-specific frameworks from healthcare, education, cybersecurity, autonomous systems, and environmental modeling. Ethical terms were tokenized and binary-coded to determine recurrence across documents and inform the value ranking system. Dimensional coherence was established using unsupervised classification techniques, including Principal Component Analysis (PCA), Factor Analysis (FA), and Latent Dirichlet Allocation (LDA), which helped reduce conceptual overlap and surface underlying value clusters. GPT-4 Turbo was employed as a natural language processing tool to verify alignment across texts, synthesize thematic trends, and assist in pattern recognition during value classification.\nWe applied PCA to reduce conceptual redundancy and identify dominant ethical axes within the corpus. This allowed us to distinguish high-frequency terms that also contributed explanatory structure-ensuring that values like trust, transparency, non-maleficence, and privacy emerged not only as common, but as organizing principles. FA surfaced latent moral dimensions, revealing how clusters of values-such as autonomy, consent, and dignity-coalesced into deeper normative commitments. LDA was used to algorithmically detect topic groupings, confirming that ethical principles consistently co-occurred in recognizable thematic patterns across governance sectors. Together, these techniques enabled the formalization of a value canon grounded in statistical weight and moral coherence-supporting both shared responsibility and ethical responsibility as pillars of operational AI alignment.\nWhile prior mappings such as Jobin et al. [3] (84 documents) and Fjeld et al. [4] (36 documents) identified clusters of ethical convergence, the AI Moral Code advances this work by introducing a stratified frequency model that formally differentiates canonical values based on statistical recurrence. Justice, transparency, responsibility, nonmaleficence, and inclusivity emerged as the five most persistent cross-sectoral values through cross-validation against sectoral corpora and semantic alignment. Validation was supported by convergence with analytic frameworks (Floridi et al. [5]) and institutional standards (OECD [2], IEEE [1]). These values not only reflect empirical durability but also serve distinct governance functions within the NRBC architecture, enabling structural coherence without enforcing normative convergence. Their stratification was further reinforced through crossdomain validation using the AI Moral Code 5\u00d75 matrix, confirming their foundational role in ethical AI system design.",
            "publication_ref": [
                "b2",
                "b3",
                "b4",
                "b1",
                "b0"
            ],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "V. RESULTS AND ANALYSIS",
            "text": "These findings validate the AI Moral Code's adaptability across domains while identifying areas requiring refinement, particularly in transparency and responsibility. The table summarizes simulation outcomes evaluating the AI Moral Code across four high-impact domains: healthcare AI, autonomous systems, educational AI, and climate modeling. Each domain was assessed using metrics tied to its respective canonical values-such as trust, privacy, safety, and transparency.\nDespite strong performance indicators-including benchmark safety in autonomous systems, high trust in healthcare diagnostics, and 92% accuracy in climate modeling-critical areas for refinement emerged. These include enhancing transparency in autonomous systems, improving consent traceability in clinical tools, and mitigating latent bias in educational AI classification models.\nWhile canonical values formed the core evaluation for simulation testing, several sub-canonical and support-layer values proved indispensable in revealing operational fault lines. Notably, explainability-identified by Fjeld et al. as a key operational construct [4], and by Floridi et al. through the principle of explicability [5]-surfaced as a recurrent point of ethical tension in both healthcare and education scenarios. These cases affirm that sub-canonical values are not secondary, but function as ethical scaffolding, supporting the operational coherence of the canon.\nImplementation of the AI Moral Code has extended into applied contexts. In 2024, as part of an NSA-supported initiative (Grant H98230-22-1-0329) at Norwich University in partnership with the University of Cincinnati, the framework was deployed within National Centers of Academic Excellence-Cyber (NCAE-C) Co-Op programs. Canonical values and NRBC structures were embedded into cybersecurity team formation workflows, supporting both instructional efficacy and operational decision-making. These early deployments confirm that moral reasoning, AI-assisted coaching, and value alignment are not merely conceptual, but can be systematically embedded into real-world education and governance environments.",
            "publication_ref": [
                "b3",
                "b4"
            ],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "VI. DISCUSSION",
            "text": "",
            "publication_ref": [],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "A. Comparative Analysis",
            "text": "The AI Moral Code aligns with IEEE's Ethically Aligned Design [1], the EU AI Act [8], and NIST's AI Risk Management Framework [9]. Unlike static guidelines, the AI Moral Code integrates empirical validation through simulation testing, enhancing its operational applicability.\nThe use of GPT-4 Turbo in this framework constitutes a novel application of large language models-not as generators of ethical insight, but as structured simulation agents embedded within a value-aligned governance architecture. Unlike typical deployments focused on conversational modeling or surfacelevel alignment, this implementation binds model outputs to a codified ethical canon, enabling rigorous stress-testing of moral reasoning under constraint.\nWhile the Conceptual layer of the NRBC architecture was initially conceived to reflect high-level moral constructs-such as dignity, epistemic humility, and sustainability-its role has evolved through implementation. Within the AI Moral Code, the Conceptual now functions as a proposed ethical scaffold, guiding design logic, behavioral modeling, and functional goals in AI agent development. Though assessed primarily through simulation and early-stage deployment in cybersecurity education, its structure is positioned for generalization. Broader institutional replication will be necessary to confirm its adaptability as a durable systems layer.\nBy embedding value classification and simulation testing within the NRBC architecture, the AI Moral Code proposes a methodology for bridging the gap between philosophical alignment and systems design. Its development lifecyclecurrently under refinement-aims to support iterative implementation, value drift monitoring, and post-deployment ethical review.\nWhile not yet a universal computational architecture, it offers a structured pathway toward embedding ethical values in both policy and runtime system behavior. Furthermore, the moral domains and subdomains (Core, Social, Cultural, Futuristic), nested within NRBC's stratified architecture, enable both granular analysis and conceptual extensibility. This layered system-grounded in canonical frequency thresholds, informed by primary source alignment (IEEE, OECD, Floridi, Fjeld, Jobin, Bonnici), and mapped through semantic coherence-serves not only as an ethical classification model but as a governance design framework responsive to institutional and cultural variation.\nThe AI Moral Code thus moves beyond ethics-byconsensus toward ethics-by-design. It offers an original ontology-synthesizing decades of AI ethics discourse into an executable framework. In doing so, it affirms its own intellectual authorship-not through branding, but through scholastic construction, methodological discipline, and structural innovation.",
            "publication_ref": [
                "b0",
                "b7",
                "b8"
            ],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "VII. CROSS-CULTURAL ETHICAL CONSIDERATIONS",
            "text": "AI ethics and governance extend beyond technical AI ethics and governance extend beyond technical considerations, drawing deeply on cultural, philosophical, and institutional foundations. The AI Moral Code addresses this complexity through a structured ethical framework grounded in the NRBC architecture and encompassing four interconnected domains: Core, Social, Cultural, and Futuristic. This approach supports both precision in ethical analysis and adaptability in governance, enabling alignment with diverse regional policy environments.\nEarlier cross-regional ethics mappings by Jobin et al. [3], Fjeld et al. [4], and Bonnici et al. [6] identified patterns of convergence within global AI governance frameworks. Building on this groundwork, the AI Moral Code introduces a detailed taxonomy structured through frequency analysis, sectoral distribution, and semantic alignment. Canonical values are prioritized based on recurrence, while their application is organized through moral domain classification and governance function assignment. This ensures these values can be interpreted and applied within distinct institutional systems.",
            "publication_ref": [
                "b2",
                "b3",
                "b5"
            ],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "A. Philosophical Divergence and Ethical Foundations",
            "text": "Ethical reasoning is culturally embedded. Distinct philosophical traditions shape how societies define autonomy, transparency, responsibility, and institutional legitimacy in artificial intelligence:\n1. In Western contexts, informed by Enlightenment principles such as autonomy and procedural fairness, AI transparency requirements often emphasize user consent and explainability. For example, under the EU's General Data Protection Regulation (GDPR) [10], organizations deploying AI must provide clear explanations of automated decision-making processes, ensuring individuals retain agency over how their data is used. This leads to governance mechanisms that prioritize regulatory compliance and individual rights.\n2. In East Asian contexts, influenced by collective traditions such as Confucianism and Daoism, AI systems are often integrated into public infrastructure to support long-term development goals and social coordination. For example, China's national ethical guidelines emphasize collective benefit through AI deployment in public administration and urban systems [11]. These frameworks emphasize alignment with societal cohesion over individual autonomy.\nThe AI Moral Code's Conceptual and Cultural domains reflect this diversity through a structured but adaptable design. Values such as dignity, epistemic humility, solidarity, and sustainability are retained in the canonical value set due to their broad cross-sectoral relevance. By linking these values to domain-specific governance functions, the framework enables context-sensitive application without compromising structural consistency.",
            "publication_ref": [
                "b9",
                "b10"
            ],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "B. Variations in Governance Models",
            "text": "AI governance models differ not only in principle but also in regulatory implementation. Ethical values are expressed according to distinct legal traditions, institutional mechanisms, and political structures.\nThe European Union employs a precautionary regulatory model, as exemplified by the AI Act, which prioritizes risk assessment and rights-based protections before system deployment. The Act mandates transparency and accountability in high-risk AI systems, including biometric surveillance tools [8].\nThe United States follows a reactive governance model, generally allowing innovation to advance before codifying ethical or legal constraints. Sectoral policies, such as the 2024 memorandum from the White House Office of Management and Budget (OMB), emphasize responsible AI procurement within federal agencies, alongside protections for privacy and civil liberties [12], [13].\nChina's model incorporates AI into long-range strategic planning, aligning system deployment with national development priorities and coordinated social outcomes as articulated in its national ethical norms [11].\nThe timing of ethical intervention also varies by region. Western models tend to focus on development-stage ethics, emphasizing data quality, fairness in training, and transparency in system design. In contrast, East Asian frameworks emphasize deployment-stage alignment, focusing on how AI outcomes serve public priorities and align with societal mandates [14].",
            "publication_ref": [
                "b7",
                "b11",
                "b10",
                "b12"
            ],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "C. Strategic and Geopolitical Dimensions",
            "text": "AI increasingly operates as a geopolitical instrument, with governance strategies reflecting divergent national interests:\n1. The U.S.-China relationship underscores fundamental contrasts in openness, regulatory sovereignty, and ideological framing [15].\n2. The European Union seeks to establish international standards through digital sovereignty, embedding AI regulation into legal and constitutional frameworks [8]. 3. South Korea and Japan apply hybrid strategies-balancing risk-sensitive ethical safeguards with innovation incentives and regional interoperability [16], [17].",
            "publication_ref": [
                "b14",
                "b7",
                "b15",
                "b16"
            ],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "International coordination mechanisms such as the United",
            "text": "Nations' Global Digital Compact further underscore the need for ethical frameworks that are structurally rigorous yet adaptable across jurisdictions [18].\nThe AI Moral Code does not present universal values as immutable dictates. Rather, it provides a structured, empirically grounded model that distinguishes canonical, sub-canonical, and architectural values through stratified classification. Values such as inclusivity, innovation, and sustainability are presented not as static imperatives, but as governance-aligned elementspositioned by domain, verified through recurrence, and adaptable to diverse policy environments.",
            "publication_ref": [
                "b17"
            ],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "VIII. INTEGRATION INTO THE AI MORAL CODE",
            "text": "The AI Moral Code operationalizes a structured approach to global AI governance by addressing regional, philosophical, and institutional variation through an ethical architecture grounded in the NRBC architecture. Its integration strategy emphasizes model consistency over cultural uniformity, ensuring that values are not only recognized across jurisdictions but implemented in alignment with existing governance systems.\nThree structural components support this integration: value harmonization, regulatory alignment, and deployable governance architecture.",
            "publication_ref": [],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "A. Harmonization of Ethical Values",
            "text": "While cultural and political systems differ, several ethical values-such as trust, transparency, responsibility, nonmaleficence, and privacy-consistently appear across sectors and regions. These canonical values serve as anchoring elements within the AI Moral Code. The framework harmonizes these values by ensuring they are:\n1. Contextually adaptable: Each value is anchored within a domain-specific governance function (Normative, Regulatory, Behavioral, or Conceptual), allowing localized interpretation without losing systemic coherence. This ensures values can adapt to cultural, legal, or sectoral differences while maintaining a stable ethical framework.\n2. Operationalized through design: Each value is translated into standards measurable through empirical simulation, performance benchmarks, and ethical stress testing.",
            "publication_ref": [],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "Balanced across moral logics:",
            "text": "The framework supports both autonomy-driven individual rights (common in Western democracies) and outcome-based collective ethics (as seen in East Asian systems), without subsuming one under the other.",
            "publication_ref": [],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "B. Alignment with Regulatory Frameworks",
            "text": "Scalability and enforceability require that the AI Moral Code interface directly with institutional and legal systems. Rather than positioning itself as a replacement for regulatory mandates, the framework is designed to map onto them structurally:\n1. Canonical values are cross-referenced with policy instruments, including the EU's AI Act, the U.S. NIST AI Risk Management Framework, and China's national AI ethics norms.\n2. Ethical benchmarks are regionalized, with the NRBC architecture serving as a middle layer that translates principles into actionable policy recommendations aligned with existing oversight mechanisms.\n3. Version control and adaptive governance are embedded in the framework, allowing updates to be incorporated as regulatory environments mature or diverge.",
            "publication_ref": [],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "C. Practical Implementation in AI Governance",
            "text": "To move beyond normative guidance, the AI Moral Code embeds its ethical structure into governance practice. This is not a conceptual gesture-it is a requirement for deployment.\n1. Design-stage integration: Values are embedded directly into AI system architecture, supporting ethical alignment during development, testing, and deployment phases.",
            "publication_ref": [],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "Evaluation metrics:",
            "text": "The framework establishes performance-based assessments validated through scenario testing, simulation environments, and value-specific KPIs.\n3. Institutional coordination: Deployment scenarios include multi-stakeholder roles-from technical design teams to ethics boards and public oversight bodiesensuring shared responsibility across the AI lifecycle.",
            "publication_ref": [],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "IX. TOWARDS A UNIFIED ETHICAL AI FRAMEWORK",
            "text": "The AI Moral Code consolidates frequency-grounded values, stratified moral domains, and governance functions into a unified model that is enforceable, adaptable, and scalable. Rather than advancing a universal doctrine, it presents an operational architecture capable of aligning diverse ethical systems while preserving structural rigor without ethical reductionism.\nCore values are formalized not through assumed consensus, but through recurrence analysis and architectural assignment. By embedding these statistically persistent ethical themes into a layered framework-organized by function and domain-the model enables structural consistency while preserving normative diversity. Global coordination becomes feasible without requiring ethical convergence. This approach supports cultural expression, institutional alignment, and policy integration without sacrificing definitional clarity. The framework's capacity to translate values across governance contexts-through the NRBC architecture and domain-specific roles-ensures adaptability across regulatory ecosystems.\nAs AI continues to reshape critical infrastructure, labor systems, education, and geopolitics, the next phase of this work is practical: to extend the AI Moral Code into formal policy instruments, industry compliance mechanisms, and operational AI risk mitigation strategies. Its integration into standards, procurement protocols, and oversight frameworks will determine not only its institutional legitimacy, but its long-term contribution to responsible AI development. ",
            "publication_ref": [],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "X. CONCLUSION AND FUTURE WORK",
            "text": "The AI Moral Code presents a scalable, structured, and empirically grounded framework for ethical AI governance. It introduces a layered value architecture-defined through recurrence analysis, operationalized through the NRBC architecture, and tested through structured simulations. Its contribution lies in its ability to synthesize philosophical traditions, regulatory systems, and deployment scenarios into a unified ethical framework that is both technically actionable and culturally adaptable.\nWhile simulation testing has demonstrated the framework's internal coherence and contextual responsiveness, further validation is required. The next phase of development is not universal implementation, but domain-specific replication, stakeholder evaluation, and longitudinal assessment. Future research should focus on:\n1. Expanding real-world pilot deployments across sectors and regions to evaluate longitudinal ethical impact.\n2. Refining transparency metrics and traceability mechanisms to support cross-stakeholder accountability.\n3. Addressing emergent dilemmas associated with AGI development, multi-agent collaboration, and AI-human moral co-decision systems.\nPortions of the AI Moral Code framework have already been implemented and tested within the National Centers of Academic Excellence -Cyber (NCAE-C) programs, including a 2024 grant-supported initiative at Norwich University in partnership with the University of Cincinnati. These early deployments provide evidence of the framework's educational and practical applicability in high-stakes environments such as cybersecurity team formation [19]. This is not a conclusion of principle-it is a transition of method. The AI Moral Code must now be refined through realworld alignment, policy instrumentation, and multiinstitutional validation. Its value will ultimately be determined not by its declaration, but by its deployment.",
            "publication_ref": [
                "b18"
            ],
            "figure_ref": [],
            "table_ref": []
        }
    ],
    "references": [
        {
            "ref_id": "b0",
            "title": "Ethically Aligned Design: A Vision for Prioritizing Human Well-being with Autonomous and Intelligent Systems, 1st ed. IEEE",
            "journal": "",
            "year": "2019",
            "authors": ""
        },
        {
            "ref_id": "b1",
            "title": "OECD Framework for the Classification of AI Systems",
            "journal": "OECD Publishing",
            "year": "2022",
            "authors": ""
        },
        {
            "ref_id": "b2",
            "title": "The global landscape of AI ethics guidelines",
            "journal": "Nature Machine Intelligence",
            "year": "2019",
            "authors": "A Jobin; M Ienca; E Vayena"
        },
        {
            "ref_id": "b3",
            "title": "Principled Artificial Intelligence: Mapping Consensus in Ethical and Rights-Based Approaches to Principles for AI",
            "journal": "",
            "year": "2020-01",
            "authors": "J Fjeld; N Achten; H Hilligoss; A Nagy; M Srikumar"
        },
        {
            "ref_id": "b4",
            "title": "AI4People-An ethical framework for a good AI society: Opportunities, risks, principles, and recommendations",
            "journal": "Minds and Machines",
            "year": "2018",
            "authors": "L Floridi; J Cowls; M Beltrametti; R Chatila; P Chazerand; V Dignum"
        },
        {
            "ref_id": "b5",
            "title": "Artificial Intelligence and Human Rights: Opportunities and Risks",
            "journal": "",
            "year": "2021",
            "authors": "J P Bonnici; S M West; J Cowls; D B Taylor"
        },
        {
            "ref_id": "b6",
            "title": "The AI Moral Code: Ethical Principles for AI Governance and Accountability",
            "journal": "",
            "year": "",
            "authors": "R J Hinrichs"
        },
        {
            "ref_id": "b7",
            "title": "Proposal for a Regulation of the European Parliament and of the Council laying down harmonized rules on artificial intelligence (Artificial Intelligence Act) and amending certain Union legislative acts",
            "journal": "",
            "year": "2021",
            "authors": " "
        },
        {
            "ref_id": "b8",
            "title": "Artificial Intelligence Risk Management Framework (AI RMF) 1.0, NIST",
            "journal": "",
            "year": "2023",
            "authors": ""
        },
        {
            "ref_id": "b9",
            "title": "Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (General Data Protection Regulation)",
            "journal": "Official Journal of the European Union",
            "year": "2016-05",
            "authors": ""
        },
        {
            "ref_id": "b10",
            "title": "National Governance Committee for the New Generation Artificial Intelligence, Ethical Norms for the New Generation Artificial Intelligence",
            "journal": "",
            "year": "2021",
            "authors": ""
        },
        {
            "ref_id": "b11",
            "title": "Advancing Governance, Innovation, and Risk Management for Agency Use of Artificial Intelligence, M",
            "journal": "Office of Management and Budget",
            "year": "2024-04",
            "authors": ""
        },
        {
            "ref_id": "b12",
            "title": "Personalized education and artificial intelligence in the United States, China, and India: A systematic review using a Human-In-The-Loop model",
            "journal": "Comput. Educ. Artif. Intell",
            "year": "",
            "authors": "A Bhutoria"
        },
        {
            "ref_id": "b13",
            "title": "\u00a92025 IEEE Authorized licensed use limited to: Kennesaw State University. Downloaded on October 13,2025 at 18:42:18 UTC from IEEE Xplore",
            "journal": "",
            "year": "",
            "authors": ""
        },
        {
            "ref_id": "b14",
            "title": "AI Policy: Global Perspectives",
            "journal": "AI & Society",
            "year": "2020",
            "authors": "S Huang"
        },
        {
            "ref_id": "b15",
            "title": "Government of South Korea, Framework Act on the Development of Artificial Intelligence and Establishment of Trust Foundation, National Assembly of South Korea",
            "journal": "",
            "year": "2024-12",
            "authors": ""
        },
        {
            "ref_id": "b16",
            "title": "Japan's approach to AI regulation and its impact on the 2023 G7 presidency",
            "journal": "",
            "year": "2023",
            "authors": " Csis"
        },
        {
            "ref_id": "b17",
            "title": "Our Common Agenda Policy Brief 5: A Global Digital Compact -An Open, Free and Secure Digital Future for All, United Nations Publications",
            "journal": "United Nations",
            "year": "2024",
            "authors": ""
        },
        {
            "ref_id": "b18",
            "title": "Harnessing AI for Ethical Team Formation in Cybersecurity Education",
            "journal": "",
            "year": "2025-09",
            "authors": "R J Hinrichs; A Ayala; C Hartman; R Hoyt; S Stoll"
        }
    ],
    "figures": [
        {
            "figure_label": "8",
            "figure_type": "figure",
            "figure_id": "fig_0",
            "figure_caption": "979- 8 -83315-2472-2/25/$31.00 \u00a92025 IEEE Authorized licensed use limited to: Kennesaw State University. Downloaded on October 13,2025 at 18:42:18 UTC from IEEE Xplore. Restrictions apply.",
            "figure_data": ""
        },
        {
            "figure_label": "I",
            "figure_type": "table",
            "figure_id": "tab_1",
            "figure_caption": "",
            "figure_data": ".SIMULATION OUTCOME SUMMARY BY DOMAINUse CaseCanonical Values TestedResults AchievedAreas for RefinementImproveHealthcareTrust, Privacy,Trust: 86%,explainabilityAITransparencySafety: Highand consentvisibilityAutonomous SystemsNon-maleficence, ResponsibilityCollision rate < 0.05% (benchmark met)Increasing traceability of ethical logicEducational AITransparency, Responsibility78% recommendation transparency achievedAddress implicit bias in classificationClimate Modeling AITransparency, ResponsibilityAccuracy: 92%Improved clarity for policymakers"
        }
    ],
    "formulas": [],
    "doi": "10.1109/ICAD65464.2025.11114041"
}